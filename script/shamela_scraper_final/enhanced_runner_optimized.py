# -*- coding: utf-8 -*-
"""
Enhanced Shamela Runner Optimized - Ø³ÙƒØ±Ø¨Øª ØªØ´ØºÙŠÙ„ Ù…Ø­Ø³Ù† Ù„Ù„Ù…ÙƒØªØ¨Ø© Ø§Ù„Ø´Ø§Ù…Ù„Ø©
ÙŠØ¬Ù…Ø¹ Ø¬Ù…ÙŠØ¹ Ø§Ù„ÙˆØ¸Ø§Ø¦Ù Ø§Ù„Ù…Ø­Ø³Ù†Ø© ÙÙŠ ÙˆØ§Ø¬Ù‡Ø© ÙˆØ§Ø­Ø¯Ø© Ø³Ù‡Ù„Ø© Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…Ø¹ ØªØ­Ø³ÙŠÙ†Ø§Øª Ø§Ù„Ø£Ø¯Ø§Ø¡

Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…Ø­Ø³Ù†Ø©:
- Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙƒØªØ¨ Ù…Ø¹ Ø¬Ù…ÙŠØ¹ Ø§Ù„ØªØ­Ø³ÙŠÙ†Ø§Øª ÙˆØ§Ù„ØªÙˆØ§Ø²ÙŠ
- Ø­ÙØ¸ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø­Ø³Ù†Ø© Ù…Ø¹ Batch Processing
- Ø¥Ù†Ø´Ø§Ø¡ ØªÙ‚Ø§Ø±ÙŠØ± Ø´Ø§Ù…Ù„Ø©
- Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø£Ø®Ø·Ø§Ø¡ Ø§Ù„Ù…Ø­Ø³Ù†Ø©
- Ù†Ø¸Ø§Ù… Ø§Ù„Ø§Ø³ØªØ¦Ù†Ø§Ù Ø§Ù„Ø¢Ù…Ù†
- ØªØ­Ø³ÙŠÙ†Ø§Øª Ø§Ù„Ø°Ø§ÙƒØ±Ø© ÙˆØ§Ù„Ø£Ø¯Ø§Ø¡
"""

import os
import sys
import json
import logging
import argparse
import hashlib
from datetime import datetime
from pathlib import Path
from logging.handlers import RotatingFileHandler
from typing import Dict, Any, Optional

# Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ù…Ø¬Ù„Ø¯ Ø§Ù„Ø­Ø§Ù„ÙŠ Ù„Ù„Ù€ path
current_dir = os.path.dirname(os.path.abspath(__file__))
sys.path.insert(0, current_dir)

try:
    from enhanced_shamela_scraper_optimized import (
        scrape_enhanced_book, save_enhanced_book_to_json,
        OptimizationConfig as ScraperConfig
    )
    from enhanced_database_manager_optimized import (
        EnhancedShamelaDatabaseManagerOptimized
    )
except ImportError:
    # Fallback to original versions if optimized not available
    try:
        from enhanced_shamela_scraper import scrape_enhanced_book, save_enhanced_book_to_json
        from enhanced_database_manager import EnhancedShamelaDatabaseManager as EnhancedShamelaDatabaseManagerOptimized
        ScraperConfig = None
        DatabaseConfig = None
    except ImportError as e:
        print(f"Ø®Ø·Ø£ ÙÙŠ Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„ÙˆØ­Ø¯Ø§Øª: {e}")
        print("ØªØ£ÙƒØ¯ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ù…Ù„ÙØ§Øª enhanced_shamela_scraper.py Ùˆ enhanced_database_manager.py")
        sys.exit(1)

# Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ù…Ø­Ø³Ù†
def setup_optimized_logging(log_level: str = 'INFO', max_bytes: int = 10*1024*1024, backup_count: int = 5):
    """Ø¥Ø¹Ø¯Ø§Ø¯ Ù†Ø¸Ø§Ù… Ø§Ù„ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ù…Ø­Ø³Ù† Ù…Ø¹ RotatingFileHandler"""
    logger = logging.getLogger()
    logger.setLevel(getattr(logging, log_level.upper()))
    
    # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø©
    for handler in logger.handlers[:]:
        logger.removeHandler(handler)
    
    # Ù…Ø¹Ø§Ù„Ø¬ Ø§Ù„Ù…Ù„Ù Ø§Ù„Ø¯ÙˆØ§Ø±
    file_handler = RotatingFileHandler(
        'enhanced_shamela_runner_optimized.log',
        maxBytes=max_bytes,
        backupCount=backup_count,
        encoding='utf-8'
    )
    file_handler.setLevel(getattr(logging, log_level.upper()))
    
    # Ù…Ø¹Ø§Ù„Ø¬ ÙˆØ­Ø¯Ø© Ø§Ù„ØªØ­ÙƒÙ…
    console_handler = logging.StreamHandler()
    console_handler.setLevel(logging.INFO)
    
    # ØªÙ†Ø³ÙŠÙ‚ Ø§Ù„Ø³Ø¬Ù„Ø§Øª
    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
    file_handler.setFormatter(formatter)
    console_handler.setFormatter(formatter)
    
    logger.addHandler(file_handler)
    logger.addHandler(console_handler)
    
    return logger

logger = setup_optimized_logging()

def print_header():
    """Ø·Ø¨Ø§Ø¹Ø© Ø±Ø£Ø³ Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬"""
    print("=" * 60)
    print("Ø³ÙƒØ±Ø¨Øª Ø§Ù„Ù…ÙƒØªØ¨Ø© Ø§Ù„Ø´Ø§Ù…Ù„Ø© Ø§Ù„Ù…Ø­Ø³Ù† - Ø§Ù„Ø¥ØµØ¯Ø§Ø± Ø§Ù„Ù…Ø·ÙˆØ±")
    print("Enhanced Shamela Scraper - Optimized Version")
    print("=" * 60)
    print()

def print_separator():
    """Ø·Ø¨Ø§Ø¹Ø© ÙØ§ØµÙ„"""
    print("-" * 60)

def create_checkpoint_file(book_id: str, progress_data: Dict[str, Any]) -> str:
    """Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù Ù†Ù‚Ø·Ø© ØªÙØªÙŠØ´ Ù„Ù„Ø§Ø³ØªØ¦Ù†Ø§Ù Ø§Ù„Ø¢Ù…Ù†"""
    checkpoint_dir = os.path.join(current_dir, "checkpoints")
    os.makedirs(checkpoint_dir, exist_ok=True)
    
    checkpoint_file = os.path.join(checkpoint_dir, f"checkpoint_{book_id}.json")
    
    with open(checkpoint_file, 'w', encoding='utf-8') as f:
        json.dump(progress_data, f, ensure_ascii=False, indent=2)
    
    return checkpoint_file

def load_checkpoint_file(book_id: str) -> Optional[Dict[str, Any]]:
    """ØªØ­Ù…ÙŠÙ„ Ù…Ù„Ù Ù†Ù‚Ø·Ø© Ø§Ù„ØªÙØªÙŠØ´"""
    checkpoint_file = os.path.join(current_dir, "checkpoints", f"checkpoint_{book_id}.json")
    
    if os.path.exists(checkpoint_file):
        try:
            with open(checkpoint_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            logger.warning(f"ÙØ´Ù„ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ù†Ù‚Ø·Ø© Ø§Ù„ØªÙØªÙŠØ´: {e}")
    
    return None

def calculate_content_hash(content: str) -> str:
    """Ø­Ø³Ø§Ø¨ hash Ù„Ù„Ù…Ø­ØªÙˆÙ‰ Ù„Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„ØªØ·Ø§Ø¨Ù‚"""
    return hashlib.sha256(content.encode('utf-8')).hexdigest()

def extract_book_full_optimized(book_id: str, max_pages: int = None, output_dir: str = None, 
                               optimization_config: Dict[str, Any] = None) -> dict:
    """
    Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙƒØªØ§Ø¨ ÙƒØ§Ù…Ù„ Ù…Ø¹ Ø¬Ù…ÙŠØ¹ Ø§Ù„ØªØ­Ø³ÙŠÙ†Ø§Øª
    """
    print(f"ğŸ” Ø¨Ø¯Ø¡ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙƒØªØ§Ø¨ Ø§Ù„Ù…Ø­Ø³Ù†: {book_id}")
    print_separator()
    
    try:
        # Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªÙƒÙˆÙŠÙ† Ø§Ù„Ù…Ø­Ø³Ù†
        config = optimization_config or {}
        
        # ØªØ­Ù‚Ù‚ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ù†Ù‚Ø·Ø© ØªÙØªÙŠØ´ Ù„Ù„Ø§Ø³ØªØ¦Ù†Ø§Ù
        checkpoint_data = None
        if config.get('resume', False):
            checkpoint_data = load_checkpoint_file(book_id)
            if checkpoint_data:
                print(f"ğŸ“‹ ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù†Ù‚Ø·Ø© ØªÙØªÙŠØ´ØŒ Ø§Ù„Ø§Ø³ØªØ¦Ù†Ø§Ù Ù…Ù† Ø§Ù„ØµÙØ­Ø© {checkpoint_data.get('last_page', 0)}")
        
        # Ø¥Ø¹Ø¯Ø§Ø¯ ÙƒÙˆÙ†ÙÙŠØº Ø§Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬
        scraper_config = None
        if ScraperConfig:
            scraper_config = ScraperConfig(
                max_workers=config.get('max_workers', 4),
                rate_limit=config.get('rate', 2.0),
                timeout=config.get('timeout', 30),
                retries=config.get('retries', 3),
                chunk_size=config.get('chunk_size', 100),
                stream_json=config.get('stream_json', False),
                resume=checkpoint_data is not None,
                skip_existing=config.get('skip_existing', False)
            )
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙƒØªØ§Ø¨
        print("ğŸ“– Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ÙƒØªØ§Ø¨...")
        if scraper_config:
            book = scrape_enhanced_book(book_id, max_pages=max_pages, 
                                      extract_content=True, config=scraper_config)
        else:
            book = scrape_enhanced_book(book_id, max_pages=max_pages, extract_content=True)
        
        # ØªØ­Ø¯ÙŠØ¯ Ù…Ø¬Ù„Ø¯ Ø§Ù„Ø¥Ø®Ø±Ø§Ø¬
        if not output_dir:
            output_dir = os.path.join(current_dir, "enhanced_books_optimized")
        
        os.makedirs(output_dir, exist_ok=True)
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ø³Ù… Ø§Ù„Ù…Ù„Ù
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"enhanced_book_{book_id}_{timestamp}.json"
        output_path = os.path.join(output_dir, filename)
        
        # Ø­ÙØ¸ Ø§Ù„ÙƒØªØ§Ø¨
        print("ğŸ’¾ Ø­ÙØ¸ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª...")
        if config.get('stream_json', False):
            # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø­ÙØ¸ Ø§Ù„Ù…Ø­Ø³Ù† Ù„Ù„Ø°Ø§ÙƒØ±Ø©
            save_enhanced_book_to_json(book, output_path, stream=True)
        else:
            save_enhanced_book_to_json(book, output_path)
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù‚Ø·Ø© ØªÙØªÙŠØ´ Ù†Ù‡Ø§Ø¦ÙŠØ©
        if config.get('resume', False):
            final_checkpoint = {
                'book_id': book_id,
                'status': 'completed',
                'total_pages': len(book.pages) if book.pages else 0,
                'completion_time': datetime.now().isoformat(),
                'output_file': output_path
            }
            create_checkpoint_file(book_id, final_checkpoint)
        
        # Ø·Ø¨Ø§Ø¹Ø© Ø§Ù„Ù†ØªØ§Ø¦Ø¬
        print("\nâœ… ØªÙ… Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙƒØªØ§Ø¨ Ø¨Ù†Ø¬Ø§Ø­!")
        print_separator()
        print(f"ğŸ“š Ø§Ù„Ø¹Ù†ÙˆØ§Ù†: {book.title}")
        print(f"ğŸ‘¨â€ğŸ“ Ø§Ù„Ù…Ø¤Ù„Ù(ÙˆÙ†): {', '.join(author.name for author in book.authors)}")
        
        if book.publisher:
            print(f"ğŸ¢ Ø§Ù„Ù†Ø§Ø´Ø±: {book.publisher.name}")
            if book.publisher.location:
                print(f"ğŸ“ Ø§Ù„Ù…ÙˆÙ‚Ø¹: {book.publisher.location}")
        
        if book.book_section:
            print(f"ğŸ“‚ Ø§Ù„Ù‚Ø³Ù…: {book.book_section.name}")
        
        if book.edition:
            edition_info = f"ğŸ“„ Ø§Ù„Ø·Ø¨Ø¹Ø©: {book.edition}"
            if book.edition_number:
                edition_info += f" (Ø±Ù‚Ù…: {book.edition_number})"
            print(edition_info)
        
        print(f"ğŸ“„ Ø¹Ø¯Ø¯ Ø§Ù„ØµÙØ­Ø§Øª: {len(book.pages) if book.pages else 0}")
        print(f"ğŸ“– Ø¹Ø¯Ø¯ Ø§Ù„ÙØµÙˆÙ„: {len(book.index) if book.index else 0}")
        print(f"ğŸ“š Ø¹Ø¯Ø¯ Ø§Ù„Ø£Ø¬Ø²Ø§Ø¡: {len(book.volumes) if book.volumes else 0}")
        print(f"ğŸ’¾ Ø­ÙÙØ¸ ÙÙŠ: {output_path}")
        
        return {
            'success': True,
            'book_id': book_id,
            'title': book.title,
            'authors': [author.name for author in book.authors],
            'total_pages': len(book.pages) if book.pages else 0,
            'total_chapters': len(book.index) if book.index else 0,
            'total_volumes': len(book.volumes) if book.volumes else 0,
            'output_file': output_path,
            'extraction_time': datetime.now().isoformat()
        }
        
    except Exception as e:
        logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙƒØªØ§Ø¨: {e}")
        print(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙƒØªØ§Ø¨: {e}")
        return {
            'success': False,
            'error': str(e),
            'book_id': book_id
        }

def save_to_database_optimized(json_path: str, db_config: dict, 
                              optimization_config: Dict[str, Any] = None) -> dict:
    """
    Ø­ÙØ¸ Ù…Ù„Ù JSON ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ø¹ Ø§Ù„ØªØ­Ø³ÙŠÙ†Ø§Øª
    """
    print(f"ğŸ’¾ Ø¨Ø¯Ø¡ Ø­ÙØ¸ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {json_path}")
    print_separator()
    
    try:
        # Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªÙƒÙˆÙŠÙ† Ø§Ù„Ù…Ø­Ø³Ù†
        config = optimization_config or {}
        
        # Ø¥Ø¹Ø¯Ø§Ø¯ ÙƒÙˆÙ†ÙÙŠØº Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        from enhanced_database_manager_optimized import OptimizationConfig as DatabaseOptimizationConfig
        db_optimization_config = DatabaseOptimizationConfig(
            batch_size=config.get('batch_size', 500),
            pool_size=config.get('connection_pool_size', 5),
            prepared_statements=True,
            fast_bulk=config.get('fast_bulk', False),
            commit_interval=config.get('commit_interval', 1000)
        )
        
        # Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        if db_optimization_config:
            db_manager = EnhancedShamelaDatabaseManagerOptimized(db_config, db_optimization_config)
        else:
            db_manager = EnhancedShamelaDatabaseManagerOptimized(db_config)
        
        db_manager.connect()
        
        # ØªØ­Ù…ÙŠÙ„ ÙˆØ­ÙØ¸ Ø§Ù„ÙƒØªØ§Ø¨
        print("ğŸ“– ØªØ­Ù…ÙŠÙ„ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ÙƒØªØ§Ø¨ Ù…Ù† JSON...")
        book = db_manager.load_enhanced_book_from_json(json_path)
        
        print("ğŸ’¾ Ø­ÙØ¸ Ø§Ù„ÙƒØªØ§Ø¨ ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª...")
        book_id = db_manager.save_complete_enhanced_book(book)
        
        # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø£Ø¯Ø§Ø¡
        if hasattr(db_manager, 'get_performance_stats'):
            stats = db_manager.get_performance_stats()
            print("\nğŸ“Š Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø£Ø¯Ø§Ø¡:")
            print(f"   - Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª: {stats.get('total_queries', 0)}")
            print(f"   - Ø¹Ù…Ù„ÙŠØ§Øª Ø§Ù„Ø¥Ø¯Ø±Ø§Ø¬ Ø§Ù„Ù…Ø¬Ù…Ø¹Ø©: {stats.get('batch_inserts', 0)}")
            print(f"   - Ù†Ø¬Ø§Ø­Ø§Øª Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª: {stats.get('cache_hits', 0)}")
            print(f"   - Ø¥Ø®ÙØ§Ù‚Ø§Øª Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª: {stats.get('cache_misses', 0)}")
        
        db_manager.disconnect()
        
        print(f"\nâœ… ØªÙ… Ø­ÙØ¸ Ø§Ù„ÙƒØªØ§Ø¨ Ø¨Ù†Ø¬Ø§Ø­! (ID: {book_id})")
        
        return {
            'success': True,
            'book_id': book_id,
            'title': book.title,
            'database_id': book_id,
            'save_time': datetime.now().isoformat()
        }
        
    except Exception as e:
        logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø­ÙØ¸ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {e}")
        print(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø­ÙØ¸ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {e}")
        return {
            'success': False,
            'error': str(e),
            'json_file': json_path
        }

def extract_and_save_book_optimized(book_id: str, max_pages: int = None, 
                                   db_config: dict = None, output_dir: str = None,
                                   optimization_config: Dict[str, Any] = None) -> dict:
    """
    Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙˆØ­ÙØ¸ ÙƒØªØ§Ø¨ ÙƒØ§Ù…Ù„ Ù…Ø¹ Ø¬Ù…ÙŠØ¹ Ø§Ù„ØªØ­Ø³ÙŠÙ†Ø§Øª
    """
    print_header()
    
    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙƒØªØ§Ø¨
    extract_result = extract_book_full_optimized(book_id, max_pages, output_dir, optimization_config)
    
    if not extract_result['success']:
        return extract_result
    
    # Ø­ÙØ¸ ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¥Ø°Ø§ ØªÙ… ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª
    if db_config:
        print_separator()
        save_result = save_to_database_optimized(extract_result['output_file'], db_config, optimization_config)
        
        if save_result['success']:
            extract_result.update({
                'database_id': save_result['book_id'],
                'database_save_time': save_result['save_time']
            })
        else:
            extract_result['database_error'] = save_result['error']
    
    return extract_result

def create_database_tables_optimized(db_config: dict, optimization_config: Dict[str, Any] = None) -> dict:
    """
    Ø¥Ù†Ø´Ø§Ø¡ Ø¬Ø¯Ø§ÙˆÙ„ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ø¹ Ø§Ù„ÙÙ‡Ø§Ø±Ø³ Ø§Ù„Ù…Ø­Ø³Ù†Ø©
    """
    print("ğŸ”§ Ø¥Ù†Ø´Ø§Ø¡ Ø¬Ø¯Ø§ÙˆÙ„ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª...")
    print_separator()
    
    try:
        config = optimization_config or {}
        
        # Ø¥Ø¹Ø¯Ø§Ø¯ ÙƒÙˆÙ†ÙÙŠØº Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        from enhanced_database_manager_optimized import OptimizationConfig as DatabaseOptimizationConfig
        db_optimization_config = DatabaseOptimizationConfig(
            batch_size=config.get('batch_size', 500),
            pool_size=config.get('connection_pool_size', 5),
            prepared_statements=True,
            fast_bulk=config.get('fast_bulk', False)
        )
        
        if db_optimization_config:
            db_manager = EnhancedShamelaDatabaseManagerOptimized(db_config, db_optimization_config)
        else:
            db_manager = EnhancedShamelaDatabaseManagerOptimized(db_config)
        
        db_manager.connect()
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ø¬Ø¯Ø§ÙˆÙ„
        print("ğŸ“‹ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ø¬Ø¯Ø§ÙˆÙ„ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©...")
        db_manager.create_tables()
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ÙÙ‡Ø§Ø±Ø³ Ø§Ù„Ù…Ø­Ø³Ù†Ø©
        if hasattr(db_manager, 'create_optimized_indexes'):
            print("ğŸ” Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ÙÙ‡Ø§Ø±Ø³ Ø§Ù„Ù…Ø­Ø³Ù†Ø©...")
            db_manager.create_optimized_indexes()
        
        db_manager.disconnect()
        
        print("\nâœ… ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ø¬Ø¯Ø§ÙˆÙ„ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ù†Ø¬Ø§Ø­!")
        
        return {
            'success': True,
            'message': 'ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ø¬Ø¯Ø§ÙˆÙ„ ÙˆØ§Ù„ÙÙ‡Ø§Ø±Ø³ Ø¨Ù†Ø¬Ø§Ø­',
            'creation_time': datetime.now().isoformat()
        }
        
    except Exception as e:
        logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ø¬Ø¯Ø§ÙˆÙ„: {e}")
        print(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ø¬Ø¯Ø§ÙˆÙ„: {e}")
        return {
            'success': False,
            'error': str(e)
        }

def get_database_stats_optimized(book_id: int, db_config: dict, 
                                 optimization_config: Dict[str, Any] = None) -> dict:
    """
    Ø¹Ø±Ø¶ Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª ÙƒØªØ§Ø¨ Ù…Ù† Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ø¹ ØªØ­Ø³ÙŠÙ†Ø§Øª Ø§Ù„Ø£Ø¯Ø§Ø¡
    """
    print(f"ğŸ“Š Ø¬Ù„Ø¨ Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„ÙƒØªØ§Ø¨: {book_id}")
    print_separator()
    
    try:
        config = optimization_config or {}
        
        # Ø¥Ø¹Ø¯Ø§Ø¯ ÙƒÙˆÙ†ÙÙŠØº Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        from enhanced_database_manager_optimized import OptimizationConfig as DatabaseOptimizationConfig
        db_optimization_config = DatabaseOptimizationConfig(
            pool_size=config.get('connection_pool_size', 5),
            prepared_statements=True
        )
        
        if db_optimization_config:
            db_manager = EnhancedShamelaDatabaseManagerOptimized(db_config, db_optimization_config)
        else:
            db_manager = EnhancedShamelaDatabaseManagerOptimized(db_config)
        
        db_manager.connect()
        
        # Ø¬Ù„Ø¨ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„ÙƒØªØ§Ø¨ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
        book_query = """
        SELECT b.id, b.title, b.shamela_id, b.total_pages, b.total_volumes,
               p.name as publisher_name, bs.name as section_name
        FROM books b
        LEFT JOIN publishers p ON b.publisher_id = p.id
        LEFT JOIN book_sections bs ON b.book_section_id = bs.id
        WHERE b.id = %s
        """
        
        cursor = db_manager.connection.cursor(dictionary=True)
        cursor.execute(book_query, (book_id,))
        book_info = cursor.fetchone()
        
        if not book_info:
            return {
                'success': False,
                'error': f'Ø§Ù„ÙƒØªØ§Ø¨ ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯: {book_id}'
            }
        
        # Ø¬Ù„Ø¨ Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ù…Ø¤Ù„ÙÙŠÙ†
        authors_query = """
        SELECT a.name, a.death_year
        FROM authors a
        JOIN author_book ab ON a.id = ab.author_id
        WHERE ab.book_id = %s
        """
        cursor.execute(authors_query, (book_id,))
        authors = cursor.fetchall()
        
        # Ø¬Ù„Ø¨ Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø£Ø¬Ø²Ø§Ø¡
        volumes_query = """
        SELECT COUNT(*) as volume_count, 
               SUM(page_count) as total_volume_pages
        FROM volumes 
        WHERE book_id = %s
        """
        cursor.execute(volumes_query, (book_id,))
        volume_stats = cursor.fetchone()
        
        # Ø¬Ù„Ø¨ Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„ÙØµÙˆÙ„
        chapters_query = """
        SELECT COUNT(*) as chapter_count,
               AVG(CHAR_LENGTH(title)) as avg_title_length
        FROM chapters 
        WHERE book_id = %s
        """
        cursor.execute(chapters_query, (book_id,))
        chapter_stats = cursor.fetchone()
        
        # Ø¬Ù„Ø¨ Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„ØµÙØ­Ø§Øª
        pages_query = """
        SELECT COUNT(*) as page_count,
               AVG(CHAR_LENGTH(content)) as avg_content_length,
               SUM(CASE WHEN content IS NOT NULL AND content != '' THEN 1 ELSE 0 END) as pages_with_content,
               MIN(page_number) as min_page_number,
               MAX(page_number) as max_page_number
        FROM pages 
        WHERE book_id = %s
        """
        cursor.execute(pages_query, (book_id,))
        page_stats = cursor.fetchone()
        
        cursor.close()
        db_manager.disconnect()
        
        # Ø·Ø¨Ø§Ø¹Ø© Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
        print("ğŸ“š Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„ÙƒØªØ§Ø¨:")
        print(f"   Ø§Ù„Ø¹Ù†ÙˆØ§Ù†: {book_info['title']}")
        print(f"   Ù…Ø¹Ø±Ù Ø§Ù„Ø´Ø§Ù…Ù„Ø©: {book_info['shamela_id']}")
        if book_info['publisher_name']:
            print(f"   Ø§Ù„Ù†Ø§Ø´Ø±: {book_info['publisher_name']}")
        if book_info['section_name']:
            print(f"   Ø§Ù„Ù‚Ø³Ù…: {book_info['section_name']}")
        
        print("\nğŸ‘¨â€ğŸ“ Ø§Ù„Ù…Ø¤Ù„ÙÙˆÙ†:")
        for author in authors:
            author_info = f"   - {author['name']}"
            if author['death_year']:
                author_info += f" (Øª. {author['death_year']})"
            print(author_info)
        
        print("\nğŸ“Š Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª:")
        print(f"   Ø¹Ø¯Ø¯ Ø§Ù„Ø£Ø¬Ø²Ø§Ø¡: {volume_stats['volume_count'] or 0}")
        print(f"   Ø¥Ø¬Ù…Ø§Ù„ÙŠ ØµÙØ­Ø§Øª Ø§Ù„Ø£Ø¬Ø²Ø§Ø¡: {volume_stats['total_volume_pages'] or 0}")
        print(f"   Ø¹Ø¯Ø¯ Ø§Ù„ÙØµÙˆÙ„: {chapter_stats['chapter_count'] or 0}")
        if chapter_stats['avg_title_length']:
            print(f"   Ù…ØªÙˆØ³Ø· Ø·ÙˆÙ„ Ø¹Ù†ÙˆØ§Ù† Ø§Ù„ÙØµÙ„: {int(chapter_stats['avg_title_length'])} Ø­Ø±Ù")
        
        print(f"   Ø¹Ø¯Ø¯ Ø§Ù„ØµÙØ­Ø§Øª: {page_stats['page_count'] or 0}")
        print(f"   Ø§Ù„ØµÙØ­Ø§Øª Ø§Ù„ØªÙŠ ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ù…Ø­ØªÙˆÙ‰: {page_stats['pages_with_content'] or 0}")
        if page_stats['avg_content_length']:
            print(f"   Ù…ØªÙˆØ³Ø· Ø·ÙˆÙ„ Ø§Ù„Ù…Ø­ØªÙˆÙ‰: {int(page_stats['avg_content_length'])} Ø­Ø±Ù")
        if page_stats['min_page_number'] and page_stats['max_page_number']:
            print(f"   Ù†Ø·Ø§Ù‚ Ø£Ø±Ù‚Ø§Ù… Ø§Ù„ØµÙØ­Ø§Øª: {page_stats['min_page_number']} - {page_stats['max_page_number']}")
        
        return {
            'success': True,
            'book_info': dict(book_info),
            'authors': [dict(author) for author in authors],
            'volume_stats': dict(volume_stats),
            'chapter_stats': dict(chapter_stats),
            'page_stats': dict(page_stats),
            'stats_time': datetime.now().isoformat()
        }
        
    except Exception as e:
        logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø¬Ù„Ø¨ Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª: {e}")
        print(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø¬Ù„Ø¨ Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª: {e}")
        return {
            'success': False,
            'error': str(e),
            'book_id': book_id
        }

def run_parity_check(book_id: str, max_pages: int = None) -> dict:
    """
    ØªØ´ØºÙŠÙ„ ÙØ­Øµ Ø§Ù„ØªØ·Ø§Ø¨Ù‚ Ø¨ÙŠÙ† Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ø£ØµÙ„ÙŠØ© ÙˆØ§Ù„Ù…Ø­Ø³Ù†Ø©
    """
    print(f"ğŸ” Ø¨Ø¯Ø¡ ÙØ­Øµ Ø§Ù„ØªØ·Ø§Ø¨Ù‚ Ù„Ù„ÙƒØªØ§Ø¨: {book_id}")
    print_separator()
    
    try:
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ø£ØµÙ„ÙŠØ©
        print("ğŸ“– Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ø£ØµÙ„ÙŠØ©...")
        from enhanced_shamela_scraper import scrape_enhanced_book as original_scrape
        original_book = original_scrape(book_id, max_pages=max_pages, extract_content=True)
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ù…Ø­Ø³Ù†Ø©
        print("âš¡ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ù…Ø­Ø³Ù†Ø©...")
        optimized_book = scrape_enhanced_book(book_id, max_pages=max_pages, extract_content=True)
        
        # Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„Ù†ØªØ§Ø¦Ø¬
        print("ğŸ” Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„Ù†ØªØ§Ø¦Ø¬...")
        
        differences = []
        
        # Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
        if original_book.title != optimized_book.title:
            differences.append(f"Ø§Ù„Ø¹Ù†ÙˆØ§Ù† Ù…Ø®ØªÙ„Ù: '{original_book.title}' vs '{optimized_book.title}'")
        
        if len(original_book.authors) != len(optimized_book.authors):
            differences.append(f"Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ø¤Ù„ÙÙŠÙ† Ù…Ø®ØªÙ„Ù: {len(original_book.authors)} vs {len(optimized_book.authors)}")
        
        if len(original_book.pages or []) != len(optimized_book.pages or []):
            differences.append(f"Ø¹Ø¯Ø¯ Ø§Ù„ØµÙØ­Ø§Øª Ù…Ø®ØªÙ„Ù: {len(original_book.pages or [])} vs {len(optimized_book.pages or [])}")
        
        # Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„ÙØµÙˆÙ„ (Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ø£ØµÙ„ÙŠØ© ØªØ³ØªØ®Ø¯Ù… indexØŒ Ø§Ù„Ù…Ø­Ø³Ù†Ø© ØªØ³ØªØ®Ø¯Ù… index Ø£ÙŠØ¶Ø§Ù‹)
        original_chapters = getattr(original_book, 'index', []) or getattr(original_book, 'chapters', [])
        optimized_chapters = getattr(optimized_book, 'index', []) or getattr(optimized_book, 'chapters', [])
        
        if len(original_chapters) != len(optimized_chapters):
            differences.append(f"Ø¹Ø¯Ø¯ Ø§Ù„ÙØµÙˆÙ„ Ù…Ø®ØªÙ„Ù: {len(original_chapters)} vs {len(optimized_chapters)}")
        
        if len(original_book.volumes or []) != len(optimized_book.volumes or []):
            differences.append(f"Ø¹Ø¯Ø¯ Ø§Ù„Ø£Ø¬Ø²Ø§Ø¡ Ù…Ø®ØªÙ„Ù: {len(original_book.volumes or [])} vs {len(optimized_book.volumes or [])}")
        
        # Ù…Ù‚Ø§Ø±Ù†Ø© Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ØµÙØ­Ø§Øª
        if original_book.pages and optimized_book.pages:
            for i, (orig_page, opt_page) in enumerate(zip(original_book.pages, optimized_book.pages)):
                if orig_page.page_number != opt_page.page_number:
                    differences.append(f"Ø±Ù‚Ù… Ø§Ù„ØµÙØ­Ø© {i+1} Ù…Ø®ØªÙ„Ù: {orig_page.page_number} vs {opt_page.page_number}")
                
                if orig_page.internal_index != opt_page.internal_index:
                    differences.append(f"Ø§Ù„ÙÙ‡Ø±Ø³ Ø§Ù„Ø¯Ø§Ø®Ù„ÙŠ Ù„Ù„ØµÙØ­Ø© {i+1} Ù…Ø®ØªÙ„Ù: {orig_page.internal_index} vs {opt_page.internal_index}")
                
                if orig_page.content != opt_page.content:
                    orig_hash = calculate_content_hash(orig_page.content or "")
                    opt_hash = calculate_content_hash(opt_page.content or "")
                    if orig_hash != opt_hash:
                        differences.append(f"Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ØµÙØ­Ø© {i+1} Ù…Ø®ØªÙ„Ù (hash Ù…Ø®ØªÙ„Ù)")
        
        # Ø§Ù„Ù†ØªÙŠØ¬Ø©
        if differences:
            print("âŒ ÙØ´Ù„ ÙØ­Øµ Ø§Ù„ØªØ·Ø§Ø¨Ù‚!")
            print("Ø§Ù„Ø§Ø®ØªÙ„Ø§ÙØ§Øª Ø§Ù„Ù…ÙƒØªØ´ÙØ©:")
            for diff in differences:
                print(f"   - {diff}")
            
            return {
                'success': False,
                'parity_check': False,
                'differences': differences,
                'book_id': book_id
            }
        else:
            print("âœ… Ù†Ø¬Ø­ ÙØ­Øµ Ø§Ù„ØªØ·Ø§Ø¨Ù‚! Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ù…ØªØ·Ø§Ø¨Ù‚Ø© ØªÙ…Ø§Ù…Ø§Ù‹.")
            
            return {
                'success': True,
                'parity_check': True,
                'message': 'Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ù…ØªØ·Ø§Ø¨Ù‚Ø© ØªÙ…Ø§Ù…Ø§Ù‹ Ø¨ÙŠÙ† Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ø£ØµÙ„ÙŠØ© ÙˆØ§Ù„Ù…Ø­Ø³Ù†Ø©',
                'book_id': book_id,
                'check_time': datetime.now().isoformat()
            }
    
    except Exception as e:
        logger.error(f"Ø®Ø·Ø£ ÙÙŠ ÙØ­Øµ Ø§Ù„ØªØ·Ø§Ø¨Ù‚: {e}")
        print(f"âŒ Ø®Ø·Ø£ ÙÙŠ ÙØ­Øµ Ø§Ù„ØªØ·Ø§Ø¨Ù‚: {e}")
        return {
            'success': False,
            'error': str(e),
            'book_id': book_id
        }

def main():
    """
    Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ù„Ù„Ø³ÙƒØ±Ø¨Øª Ø§Ù„Ù…Ø­Ø³Ù†
    """
    parser = argparse.ArgumentParser(
        description="Ø³ÙƒØ±Ø¨Øª Ø§Ù„Ù…ÙƒØªØ¨Ø© Ø§Ù„Ø´Ø§Ù…Ù„Ø© Ø§Ù„Ù…Ø­Ø³Ù† - Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙˆØ­ÙØ¸ Ø§Ù„ÙƒØªØ¨ Ù…Ø¹ Ø¬Ù…ÙŠØ¹ Ø§Ù„ØªØ­Ø³ÙŠÙ†Ø§Øª",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Ø£Ù…Ø«Ù„Ø© Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…:

1. Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙƒØªØ§Ø¨ Ù…Ø¹ Ø§Ù„ØªØ­Ø³ÙŠÙ†Ø§Øª:
   python enhanced_runner_optimized.py extract 12106 --max-workers 8 --rate 3.0

2. Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙƒØªØ§Ø¨ ÙˆØ­ÙØ¸Ù‡ ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ø¹ Ø§Ù„ØªØ­Ø³ÙŠÙ†Ø§Øª:
   python enhanced_runner_optimized.py extract 12106 --db-host localhost --db-user root --db-password secret --db-name bms --batch-size 1000 --fast-bulk

3. Ø­ÙØ¸ Ù…Ù„Ù JSON Ù…ÙˆØ¬ÙˆØ¯ ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ø¹ Ø§Ù„ØªØ­Ø³ÙŠÙ†Ø§Øª:
   python enhanced_runner_optimized.py save-db enhanced_book_12106.json --db-host localhost --db-user root --db-password secret --db-name bms --batch-size 1000

4. Ø¥Ù†Ø´Ø§Ø¡ Ø¬Ø¯Ø§ÙˆÙ„ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ø¹ Ø§Ù„ÙÙ‡Ø§Ø±Ø³ Ø§Ù„Ù…Ø­Ø³Ù†Ø©:
   python enhanced_runner_optimized.py create-tables --db-host localhost --db-user root --db-password secret --db-name bms

5. Ø¹Ø±Ø¶ Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª ÙƒØªØ§Ø¨ Ù…Ù† Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª:
   python enhanced_runner_optimized.py stats 123 --db-host localhost --db-user root --db-password secret --db-name bms

6. ÙØ­Øµ Ø§Ù„ØªØ·Ø§Ø¨Ù‚ Ø¨ÙŠÙ† Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ø£ØµÙ„ÙŠØ© ÙˆØ§Ù„Ù…Ø­Ø³Ù†Ø©:
   python enhanced_runner_optimized.py parity-check 12106 --max-pages 50

7. Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ø¹ Ø§Ù„Ø§Ø³ØªØ¦Ù†Ø§Ù Ø§Ù„Ø¢Ù…Ù†:
   python enhanced_runner_optimized.py extract 12106 --resume --skip-existing
        """
    )
    
    subparsers = parser.add_subparsers(dest='command', help='Ø§Ù„Ø£ÙˆØ§Ù…Ø± Ø§Ù„Ù…ØªØ§Ø­Ø©')
    
    # Ø£Ù…Ø± Ø§Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬
    extract_parser = subparsers.add_parser('extract', help='Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙƒØªØ§Ø¨ Ù…Ù† Ø§Ù„Ù…ÙƒØªØ¨Ø© Ø§Ù„Ø´Ø§Ù…Ù„Ø©')
    extract_parser.add_argument('book_id', help='Ù…Ø¹Ø±Ù Ø§Ù„ÙƒØªØ§Ø¨ ÙÙŠ Ø§Ù„Ù…ÙƒØªØ¨Ø© Ø§Ù„Ø´Ø§Ù…Ù„Ø©')
    extract_parser.add_argument('--max-pages', type=int, help='Ø§Ù„Ø¹Ø¯Ø¯ Ø§Ù„Ø£Ù‚ØµÙ‰ Ù„Ù„ØµÙØ­Ø§Øª')
    extract_parser.add_argument('--output-dir', help='Ù…Ø¬Ù„Ø¯ Ø§Ù„Ø¥Ø®Ø±Ø§Ø¬')
    
    # Ø£Ù…Ø± Ø§Ù„Ø­ÙØ¸ ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
    save_parser = subparsers.add_parser('save-db', help='Ø­ÙØ¸ Ù…Ù„Ù JSON ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª')
    save_parser.add_argument('json_file', help='Ù…Ø³Ø§Ø± Ù…Ù„Ù JSON')
    
    # Ø£Ù…Ø± Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ø¬Ø¯Ø§ÙˆÙ„
    tables_parser = subparsers.add_parser('create-tables', help='Ø¥Ù†Ø´Ø§Ø¡ Ø¬Ø¯Ø§ÙˆÙ„ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª')
    
    # Ø£Ù…Ø± Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
    stats_parser = subparsers.add_parser('stats', help='Ø¹Ø±Ø¶ Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª ÙƒØªØ§Ø¨ Ù…Ù† Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª')
    stats_parser.add_argument('book_id', type=int, help='Ù…Ø¹Ø±Ù Ø§Ù„ÙƒØªØ§Ø¨ ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª')
    
    # Ø£Ù…Ø± ÙØ­Øµ Ø§Ù„ØªØ·Ø§Ø¨Ù‚
    parity_parser = subparsers.add_parser('parity-check', help='ÙØ­Øµ Ø§Ù„ØªØ·Ø§Ø¨Ù‚ Ø¨ÙŠÙ† Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ø£ØµÙ„ÙŠØ© ÙˆØ§Ù„Ù…Ø­Ø³Ù†Ø©')
    parity_parser.add_argument('book_id', help='Ù…Ø¹Ø±Ù Ø§Ù„ÙƒØªØ§Ø¨ ÙÙŠ Ø§Ù„Ù…ÙƒØªØ¨Ø© Ø§Ù„Ø´Ø§Ù…Ù„Ø©')
    parity_parser.add_argument('--max-pages', type=int, help='Ø§Ù„Ø¹Ø¯Ø¯ Ø§Ù„Ø£Ù‚ØµÙ‰ Ù„Ù„ØµÙØ­Ø§Øª Ù„Ù„ÙØ­Øµ')
    
    # Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª (Ù…Ø´ØªØ±ÙƒØ©)
    db_parsers = [extract_parser, save_parser, tables_parser, stats_parser]
    for subparser in db_parsers:
        subparser.add_argument('--db-host', default='localhost', help='Ø¹Ù†ÙˆØ§Ù† Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª')
        subparser.add_argument('--db-port', type=int, default=3306, help='Ù…Ù†ÙØ° Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª')
        subparser.add_argument('--db-user', default='root', help='Ø§Ø³Ù… Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…')
        subparser.add_argument('--db-password', help='ÙƒÙ„Ù…Ø© Ù…Ø±ÙˆØ± Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª')
        subparser.add_argument('--db-name', default='bms', help='Ø§Ø³Ù… Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª')
    
    # Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„ØªØ­Ø³ÙŠÙ† (Ù…Ø´ØªØ±ÙƒØ©)
    optimization_parsers = [extract_parser, save_parser, tables_parser, stats_parser, parity_parser]
    for subparser in optimization_parsers:
        # Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬
        subparser.add_argument('--max-workers', type=int, default=4, 
                             help='Ø¹Ø¯Ø¯ Ø§Ù„Ø¹Ù…Ù„ÙŠØ§Øª Ø§Ù„Ù…ØªÙˆØ§Ø²ÙŠØ© (Ø§ÙØªØ±Ø§Ø¶ÙŠ: 4)')
        subparser.add_argument('--rate', type=float, default=2.0, 
                             help='Ù…Ø¹Ø¯Ù„ Ø§Ù„Ø·Ù„Ø¨Ø§Øª ÙÙŠ Ø§Ù„Ø«Ø§Ù†ÙŠØ© (Ø§ÙØªØ±Ø§Ø¶ÙŠ: 2.0)')
        subparser.add_argument('--timeout', type=int, default=30, 
                             help='Ù…Ù‡Ù„Ø© Ø§Ù†ØªØ¸Ø§Ø± Ø§Ù„Ø·Ù„Ø¨ Ø¨Ø§Ù„Ø«ÙˆØ§Ù†ÙŠ (Ø§ÙØªØ±Ø§Ø¶ÙŠ: 30)')
        subparser.add_argument('--retries', type=int, default=3, 
                             help='Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø§Øª Ø¹Ù†Ø¯ Ø§Ù„ÙØ´Ù„ (Ø§ÙØªØ±Ø§Ø¶ÙŠ: 3)')
        subparser.add_argument('--chunk-size', type=int, default=100, 
                             help='Ø­Ø¬Ù… Ø¯ÙØ¹Ø© Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØµÙØ­Ø§Øª (Ø§ÙØªØ±Ø§Ø¶ÙŠ: 100)')
        
        # Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        subparser.add_argument('--batch-size', type=int, default=500, 
                             help='Ø­Ø¬Ù… Ø¯ÙØ¹Ø© Ø§Ù„Ø¥Ø¯Ø±Ø§Ø¬ ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª (Ø§ÙØªØ±Ø§Ø¶ÙŠ: 500)')
        subparser.add_argument('--commit-interval', type=int, default=1000, 
                             help='ÙØªØ±Ø© Ø§Ù„ØªØ²Ø§Ù… Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª (Ø§ÙØªØ±Ø§Ø¶ÙŠ: 1000)')
        subparser.add_argument('--connection-pool-size', type=int, default=5, 
                             help='Ø­Ø¬Ù… Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø§ØªØµØ§Ù„Ø§Øª (Ø§ÙØªØ±Ø§Ø¶ÙŠ: 5)')
        
        # Ø£Ø¹Ù„Ø§Ù… Ø§Ù„ØªØ­Ø³ÙŠÙ†
        subparser.add_argument('--stream-json', action='store_true', 
                             help='Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø­ÙØ¸ Ø§Ù„Ù…Ø­Ø³Ù† Ù„Ù„Ø°Ø§ÙƒØ±Ø© ÙÙŠ JSON')
        subparser.add_argument('--resume', action='store_true', 
                             help='ØªÙ…ÙƒÙŠÙ† Ø§Ù„Ø§Ø³ØªØ¦Ù†Ø§Ù Ø§Ù„Ø¢Ù…Ù† Ù…Ù† Ù†Ù‚Ø§Ø· Ø§Ù„ØªÙØªÙŠØ´')
        subparser.add_argument('--skip-existing', action='store_true', 
                             help='ØªØ®Ø·ÙŠ Ø§Ù„Ø¹Ù†Ø§ØµØ± Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø©')
        subparser.add_argument('--fast-bulk', action='store_true', 
                             help='ØªÙ…ÙƒÙŠÙ† Ø§Ù„Ø¹Ù…Ù„ÙŠØ§Øª Ø§Ù„Ù…Ø¬Ù…Ø¹Ø© Ø§Ù„Ø³Ø±ÙŠØ¹Ø© (ØªØ¹Ø·ÙŠÙ„ Ø§Ù„Ù‚ÙŠÙˆØ¯ Ù…Ø¤Ù‚ØªØ§Ù‹)')
        subparser.add_argument('--fail-fast', action='store_true', 
                             help='Ø§Ù„ØªÙˆÙ‚Ù Ø¹Ù†Ø¯ Ø£ÙˆÙ„ Ø®Ø·Ø£')
        
        # Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø³Ø¬Ù„Ø§Øª
        subparser.add_argument('--log-level', default='INFO', 
                             choices=['DEBUG', 'INFO', 'WARNING', 'ERROR'], 
                             help='Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø³Ø¬Ù„Ø§Øª (Ø§ÙØªØ±Ø§Ø¶ÙŠ: INFO)')
    
    args = parser.parse_args()
    
    if not args.command:
        parser.print_help()
        return
    
    # Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø³Ø¬Ù„Ø§Øª Ø§Ù„Ù…Ø­Ø³Ù†
    setup_optimized_logging(args.log_level)
    
    # Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
    db_config = None
    if hasattr(args, 'db_host') and any([args.db_host, args.db_user, args.db_password, args.db_name]):
        if not args.db_password:
            import getpass
            args.db_password = getpass.getpass("ÙƒÙ„Ù…Ø© Ù…Ø±ÙˆØ± Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: ")
        
        db_config = {
            'host': args.db_host,
            'port': args.db_port,
            'user': args.db_user,
            'password': args.db_password,
            'database': args.db_name
        }
    
    # Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„ØªØ­Ø³ÙŠÙ†
    optimization_config = {
        'max_workers': args.max_workers,
        'rate': args.rate,
        'timeout': args.timeout,
        'retries': args.retries,
        'chunk_size': args.chunk_size,
        'batch_size': args.batch_size,
        'commit_interval': args.commit_interval,
        'connection_pool_size': args.connection_pool_size,
        'stream_json': args.stream_json,
        'resume': args.resume,
        'skip_existing': args.skip_existing,
        'fast_bulk': args.fast_bulk,
        'fail_fast': args.fail_fast
    }
    
    try:
        if args.command == 'extract':
            result = extract_and_save_book_optimized(
                args.book_id,
                max_pages=args.max_pages,
                db_config=db_config,
                output_dir=args.output_dir,
                optimization_config=optimization_config
            )
            
            if not result['success']:
                sys.exit(1)
        
        elif args.command == 'save-db':
            if not db_config:
                print("âŒ Ø®Ø·Ø£: ÙŠØ¬Ø¨ ØªØ­Ø¯ÙŠØ¯ Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª")
                sys.exit(1)
            
            if not os.path.exists(args.json_file):
                print(f"âŒ Ø®Ø·Ø£: Ø§Ù„Ù…Ù„Ù ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯: {args.json_file}")
                sys.exit(1)
            
            result = save_to_database_optimized(args.json_file, db_config, optimization_config)
            
            if not result['success']:
                sys.exit(1)
        
        elif args.command == 'create-tables':
            if not db_config:
                print("âŒ Ø®Ø·Ø£: ÙŠØ¬Ø¨ ØªØ­Ø¯ÙŠØ¯ Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª")
                sys.exit(1)
            
            result = create_database_tables_optimized(db_config, optimization_config)
            
            if not result['success']:
                sys.exit(1)
        
        elif args.command == 'stats':
            if not db_config:
                print("âŒ Ø®Ø·Ø£: ÙŠØ¬Ø¨ ØªØ­Ø¯ÙŠØ¯ Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª")
                sys.exit(1)
            
            result = get_database_stats_optimized(args.book_id, db_config, optimization_config)
            
            if not result['success']:
                sys.exit(1)
        
        elif args.command == 'parity-check':
            result = run_parity_check(args.book_id, args.max_pages)
            
            if not result['success'] or not result.get('parity_check', False):
                sys.exit(1)
        
        print_separator()
        print("ğŸ‰ ØªÙ…Øª Ø§Ù„Ø¹Ù…Ù„ÙŠØ© Ø¨Ù†Ø¬Ø§Ø­!")
        
    except KeyboardInterrupt:
        print("\nâŒ ØªÙ… Ø¥Ù„ØºØ§Ø¡ Ø§Ù„Ø¹Ù…Ù„ÙŠØ© Ø¨ÙˆØ§Ø³Ø·Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…")
        sys.exit(1)
    except Exception as e:
        logger.error(f"Ø®Ø·Ø£ ØºÙŠØ± Ù…ØªÙˆÙ‚Ø¹: {e}")
        print(f"âŒ Ø®Ø·Ø£ ØºÙŠØ± Ù…ØªÙˆÙ‚Ø¹: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()