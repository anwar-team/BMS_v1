#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Ù†Ø¸Ø§Ù… Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª Ø§Ù„Ù…ØªÙ‚Ø¯Ù… Ù„Ù…Ø³ØªØ®Ø±Ø¬ Ø§Ù„Ø´Ø§Ù…Ù„Ø©
Advanced Caching System for Shamela Scraper

ÙŠÙˆÙØ± Ù‡Ø°Ø§ Ø§Ù„Ù†Ø¸Ø§Ù…:
- ØªØ®Ø²ÙŠÙ† Ù…Ø¤Ù‚Øª Ù„Ù„ØµÙØ­Ø§Øª Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬Ø©
- ØªØ®Ø²ÙŠÙ† Ù…Ø¤Ù‚Øª Ù„Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ÙˆØµÙÙŠØ© Ù„Ù„ÙƒØªØ¨
- Ø¥Ø¯Ø§Ø±Ø© Ø°ÙƒÙŠØ© Ù„Ù„Ø°Ø§ÙƒØ±Ø©
- Ø§Ù†ØªÙ‡Ø§Ø¡ ØµÙ„Ø§Ø­ÙŠØ© ØªÙ„Ù‚Ø§Ø¦ÙŠ
- Ø¶ØºØ· Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„ØªÙˆÙÙŠØ± Ø§Ù„Ù…Ø³Ø§Ø­Ø©
"""

import json
import gzip
import pickle
import hashlib
import time
from pathlib import Path
from typing import Dict, Any, Optional, Union, List
from dataclasses import dataclass, asdict
from threading import Lock
import logging

# Ø¥Ø¹Ø¯Ø§Ø¯ Ù†Ø¸Ø§Ù… Ø§Ù„Ø³Ø¬Ù„Ø§Øª
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class CacheEntry:
    """Ù…Ø¯Ø®Ù„ ÙÙŠ Ù†Ø¸Ø§Ù… Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª"""
    data: Any
    timestamp: float
    access_count: int = 0
    last_access: float = 0.0
    size_bytes: int = 0
    compressed: bool = False
    ttl: Optional[int] = None  # Ù…Ø¯Ø© Ø§Ù†ØªÙ‡Ø§Ø¡ Ø§Ù„ØµÙ„Ø§Ø­ÙŠØ© Ø§Ù„Ø®Ø§ØµØ© Ø¨Ù‡Ø°Ø§ Ø§Ù„Ù…Ø¯Ø®Ù„

    def __post_init__(self):
        self.last_access = time.time()
        if isinstance(self.data, (str, bytes)):
            self.size_bytes = len(self.data)
        else:
            # ØªÙ‚Ø¯ÙŠØ± ØªÙ‚Ø±ÙŠØ¨ÙŠ Ù„Ø­Ø¬Ù… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            self.size_bytes = len(str(self.data))

class AdvancedCacheSystem:
    """Ù†Ø¸Ø§Ù… Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª Ø§Ù„Ù…ØªÙ‚Ø¯Ù…"""
    
    def __init__(self, 
                 cache_dir: str = "cache",
                 max_memory_mb: int = 100,
                 max_disk_mb: int = 500,
                 default_ttl: int = 3600,  # Ø³Ø§Ø¹Ø© ÙˆØ§Ø­Ø¯Ø©
                 compression_threshold: int = 1024):  # 1KB
        """
        Ø¥Ù†Ø´Ø§Ø¡ Ù†Ø¸Ø§Ù… Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª
        
        Args:
            cache_dir: Ù…Ø¬Ù„Ø¯ Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª
            max_memory_mb: Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ù‚ØµÙ‰ Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø¨Ø§Ù„Ù…ÙŠØ¬Ø§Ø¨Ø§ÙŠØª
            max_disk_mb: Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ù‚ØµÙ‰ Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù‚Ø±Øµ Ø¨Ø§Ù„Ù…ÙŠØ¬Ø§Ø¨Ø§ÙŠØª
            default_ttl: Ù…Ø¯Ø© Ø§Ù†ØªÙ‡Ø§Ø¡ Ø§Ù„ØµÙ„Ø§Ø­ÙŠØ© Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ© Ø¨Ø§Ù„Ø«ÙˆØ§Ù†ÙŠ
            compression_threshold: Ø­Ø¯ Ø§Ù„Ø¶ØºØ· Ø¨Ø§Ù„Ø¨Ø§ÙŠØª
        """
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(exist_ok=True)
        
        self.max_memory_bytes = max_memory_mb * 1024 * 1024
        self.max_disk_bytes = max_disk_mb * 1024 * 1024
        self.default_ttl = default_ttl
        self.compression_threshold = compression_threshold
        
        # Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª ÙÙŠ Ø§Ù„Ø°Ø§ÙƒØ±Ø©
        self.memory_cache: Dict[str, CacheEntry] = {}
        self.memory_usage = 0
        
        # Ù‚ÙÙ„ Ù„Ù„Ø£Ù…Ø§Ù† ÙÙŠ Ø§Ù„Ø¨ÙŠØ¦Ø© Ù…ØªØ¹Ø¯Ø¯Ø© Ø§Ù„Ø®ÙŠÙˆØ·
        self.lock = Lock()
        
        # Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
        self.stats = {
            'hits': 0,
            'misses': 0,
            'evictions': 0,
            'disk_reads': 0,
            'disk_writes': 0
        }
        
        logger.info(f"ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ù†Ø¸Ø§Ù… Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª: {cache_dir}")
    
    def _generate_key(self, identifier: Union[str, Dict]) -> str:
        """ØªÙˆÙ„ÙŠØ¯ Ù…ÙØªØ§Ø­ ÙØ±ÙŠØ¯ Ù„Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª"""
        if isinstance(identifier, str):
            content = identifier
        else:
            content = json.dumps(identifier, sort_keys=True)
        
        return hashlib.md5(content.encode('utf-8')).hexdigest()
    
    def _compress_data(self, data: Any) -> bytes:
        """Ø¶ØºØ· Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª"""
        serialized = pickle.dumps(data)
        if len(serialized) > self.compression_threshold:
            return gzip.compress(serialized)
        return serialized
    
    def _decompress_data(self, data: bytes, compressed: bool = False) -> Any:
        """Ø¥Ù„ØºØ§Ø¡ Ø¶ØºØ· Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª"""
        if compressed:
            data = gzip.decompress(data)
        return pickle.loads(data)
    
    def _cleanup_memory(self):
        """ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø¹Ù†Ø¯ Ø§Ù„Ø­Ø§Ø¬Ø©"""
        if self.memory_usage <= self.max_memory_bytes:
            return
        
        # ØªØ±ØªÙŠØ¨ Ø§Ù„Ù…Ø¯Ø®Ù„Ø§Øª Ø­Ø³Ø¨ Ø¢Ø®Ø± ÙˆØµÙˆÙ„ (LRU)
        sorted_entries = sorted(
            self.memory_cache.items(),
            key=lambda x: x[1].last_access
        )
        
        # Ø­Ø°Ù Ø§Ù„Ù…Ø¯Ø®Ù„Ø§Øª Ø§Ù„Ø£Ù‚Ù„ Ø§Ø³ØªØ®Ø¯Ø§Ù…Ø§Ù‹
        removed_count = 0
        for key, entry in sorted_entries:
            if self.memory_usage <= self.max_memory_bytes * 0.8:  # 80% Ù…Ù† Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ù‚ØµÙ‰
                break
            
            self.memory_usage -= entry.size_bytes
            del self.memory_cache[key]
            removed_count += 1
            self.stats['evictions'] += 1
        
        if removed_count > 0:
            logger.info(f"ØªÙ… Ø­Ø°Ù {removed_count} Ù…Ø¯Ø®Ù„ Ù…Ù† Ø§Ù„Ø°Ø§ÙƒØ±Ø©")
    
    def _is_expired(self, entry: CacheEntry, ttl: Optional[int] = None) -> bool:
        """ÙØ­Øµ Ø§Ù†ØªÙ‡Ø§Ø¡ ØµÙ„Ø§Ø­ÙŠØ© Ø§Ù„Ù…Ø¯Ø®Ù„"""
        # Ø§Ø³ØªØ®Ø¯Ø§Ù… TTL Ø§Ù„Ø®Ø§Øµ Ø¨Ø§Ù„Ù…Ø¯Ø®Ù„ Ø£ÙˆÙ„Ø§Ù‹ØŒ Ø«Ù… Ø§Ù„Ù…ÙÙ…Ø±Ø±ØŒ Ø«Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠ
        effective_ttl = entry.ttl or ttl or self.default_ttl
        
        return time.time() - entry.timestamp > effective_ttl
    
    def _get_disk_path(self, key: str) -> Path:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…Ø³Ø§Ø± Ø§Ù„Ù…Ù„Ù Ø¹Ù„Ù‰ Ø§Ù„Ù‚Ø±Øµ"""
        return self.cache_dir / f"{key}.cache"
    
    def _save_to_disk(self, key: str, entry: CacheEntry):
        """Ø­ÙØ¸ Ø§Ù„Ù…Ø¯Ø®Ù„ Ø¹Ù„Ù‰ Ø§Ù„Ù‚Ø±Øµ"""
        try:
            disk_path = self._get_disk_path(key)
            compressed_data = self._compress_data(entry.data)
            
            cache_file_data = {
                'data': compressed_data,
                'timestamp': entry.timestamp,
                'access_count': entry.access_count,
                'compressed': len(compressed_data) < len(pickle.dumps(entry.data)),
                'ttl': entry.ttl
            }
            
            with open(disk_path, 'wb') as f:
                pickle.dump(cache_file_data, f)
            
            self.stats['disk_writes'] += 1
            logger.debug(f"ØªÙ… Ø­ÙØ¸ Ø§Ù„Ù…ÙØªØ§Ø­ Ø¹Ù„Ù‰ Ø§Ù„Ù‚Ø±Øµ: {key}")
            
        except Exception as e:
            logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø­ÙØ¸ Ø§Ù„Ù…ÙØªØ§Ø­ {key} Ø¹Ù„Ù‰ Ø§Ù„Ù‚Ø±Øµ: {e}")
    
    def _load_from_disk(self, key: str) -> Optional[CacheEntry]:
        """ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ø¯Ø®Ù„ Ù…Ù† Ø§Ù„Ù‚Ø±Øµ"""
        try:
            disk_path = self._get_disk_path(key)
            if not disk_path.exists():
                return None
            
            with open(disk_path, 'rb') as f:
                cache_file_data = pickle.load(f)
            
            data = self._decompress_data(
                cache_file_data['data'], 
                cache_file_data.get('compressed', False)
            )
            
            entry = CacheEntry(
                data=data,
                timestamp=cache_file_data['timestamp'],
                access_count=cache_file_data['access_count'],
                ttl=cache_file_data.get('ttl')
            )
            
            self.stats['disk_reads'] += 1
            logger.debug(f"ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…ÙØªØ§Ø­ Ù…Ù† Ø§Ù„Ù‚Ø±Øµ: {key}")
            return entry
            
        except Exception as e:
            logger.error(f"Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…ÙØªØ§Ø­ {key} Ù…Ù† Ø§Ù„Ù‚Ø±Øµ: {e}")
            return None
    
    def get(self, identifier: Union[str, Dict], ttl: Optional[int] = None) -> Optional[Any]:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù† Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª"""
        key = self._generate_key(identifier)
        
        with self.lock:
            # Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø£ÙˆÙ„Ø§Ù‹
            if key in self.memory_cache:
                entry = self.memory_cache[key]
                
                if self._is_expired(entry, ttl):
                    del self.memory_cache[key]
                    self.memory_usage -= entry.size_bytes
                    self.stats['misses'] += 1
                    return None
                
                # ØªØ­Ø¯ÙŠØ« Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„ÙˆØµÙˆÙ„
                entry.access_count += 1
                entry.last_access = time.time()
                self.stats['hits'] += 1
                
                logger.debug(f"Ø¥ØµØ§Ø¨Ø© ÙÙŠ Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ù„Ù„Ù…ÙØªØ§Ø­: {key}")
                return entry.data
            
            # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù„Ù‰ Ø§Ù„Ù‚Ø±Øµ
            entry = self._load_from_disk(key)
            if entry and not self._is_expired(entry, ttl):
                # Ø¥Ø¶Ø§ÙØ© Ø¥Ù„Ù‰ Ø§Ù„Ø°Ø§ÙƒØ±Ø©
                self.memory_cache[key] = entry
                self.memory_usage += entry.size_bytes
                
                # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø¥Ø°Ø§ Ù„Ø²Ù… Ø§Ù„Ø£Ù…Ø±
                self._cleanup_memory()
                
                entry.access_count += 1
                entry.last_access = time.time()
                self.stats['hits'] += 1
                
                logger.debug(f"Ø¥ØµØ§Ø¨Ø© Ø¹Ù„Ù‰ Ø§Ù„Ù‚Ø±Øµ Ù„Ù„Ù…ÙØªØ§Ø­: {key}")
                return entry.data
            
            self.stats['misses'] += 1
            return None
    
    def set(self, identifier: Union[str, Dict], data: Any, ttl: Optional[int] = None):
        """Ø­ÙØ¸ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª"""
        key = self._generate_key(identifier)
        
        with self.lock:
            entry = CacheEntry(data=data, timestamp=time.time(), ttl=ttl)
            
            # Ø¥Ø¶Ø§ÙØ© Ø¥Ù„Ù‰ Ø§Ù„Ø°Ø§ÙƒØ±Ø©
            if key in self.memory_cache:
                self.memory_usage -= self.memory_cache[key].size_bytes
            
            self.memory_cache[key] = entry
            self.memory_usage += entry.size_bytes
            
            # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø¥Ø°Ø§ Ù„Ø²Ù… Ø§Ù„Ø£Ù…Ø±
            self._cleanup_memory()
            
            # Ø­ÙØ¸ Ø¹Ù„Ù‰ Ø§Ù„Ù‚Ø±Øµ Ù„Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ÙƒØ¨ÙŠØ±Ø©
            if entry.size_bytes > self.compression_threshold:
                self._save_to_disk(key, entry)
            
            logger.debug(f"ØªÙ… Ø­ÙØ¸ Ø§Ù„Ù…ÙØªØ§Ø­: {key} Ù…Ø¹ TTL: {ttl}")
    
    def delete(self, identifier: Union[str, Dict]):
        """Ø­Ø°Ù Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù† Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª"""
        key = self._generate_key(identifier)
        
        with self.lock:
            # Ø­Ø°Ù Ù…Ù† Ø§Ù„Ø°Ø§ÙƒØ±Ø©
            if key in self.memory_cache:
                self.memory_usage -= self.memory_cache[key].size_bytes
                del self.memory_cache[key]
            
            # Ø­Ø°Ù Ù…Ù† Ø§Ù„Ù‚Ø±Øµ
            disk_path = self._get_disk_path(key)
            if disk_path.exists():
                disk_path.unlink()
            
            logger.debug(f"ØªÙ… Ø­Ø°Ù Ø§Ù„Ù…ÙØªØ§Ø­: {key}")
    
    def clear(self):
        """Ù…Ø³Ø­ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø®Ø²Ù†Ø© Ù…Ø¤Ù‚ØªØ§Ù‹"""
        with self.lock:
            # Ù…Ø³Ø­ Ø§Ù„Ø°Ø§ÙƒØ±Ø©
            self.memory_cache.clear()
            self.memory_usage = 0
            
            # Ù…Ø³Ø­ Ø§Ù„Ù‚Ø±Øµ
            for cache_file in self.cache_dir.glob("*.cache"):
                cache_file.unlink()
            
            logger.info("ØªÙ… Ù…Ø³Ø­ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø®Ø²Ù†Ø© Ù…Ø¤Ù‚ØªØ§Ù‹")
    
    def get_stats(self) -> Dict[str, Any]:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª"""
        total_requests = self.stats['hits'] + self.stats['misses']
        hit_rate = (self.stats['hits'] / total_requests * 100) if total_requests > 0 else 0
        
        return {
            'memory_usage_mb': self.memory_usage / 1024 / 1024,
            'memory_entries': len(self.memory_cache),
            'hit_rate_percent': hit_rate,
            'total_requests': total_requests,
            **self.stats
        }
    
    def cleanup_expired(self, ttl: Optional[int] = None):
        """ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù…Ø¯Ø®Ù„Ø§Øª Ø§Ù„Ù…Ù†ØªÙ‡ÙŠØ© Ø§Ù„ØµÙ„Ø§Ø­ÙŠØ©"""
        with self.lock:
            expired_keys = []
            
            # ÙØ­Øµ Ø§Ù„Ø°Ø§ÙƒØ±Ø©
            for key, entry in self.memory_cache.items():
                if self._is_expired(entry, ttl):
                    expired_keys.append(key)
            
            # Ø­Ø°Ù Ø§Ù„Ù…Ø¯Ø®Ù„Ø§Øª Ø§Ù„Ù…Ù†ØªÙ‡ÙŠØ© Ø§Ù„ØµÙ„Ø§Ø­ÙŠØ©
            for key in expired_keys:
                entry = self.memory_cache[key]
                self.memory_usage -= entry.size_bytes
                del self.memory_cache[key]
            
            # ÙØ­Øµ Ø§Ù„Ù‚Ø±Øµ
            disk_expired = 0
            for cache_file in self.cache_dir.glob("*.cache"):
                try:
                    key = cache_file.stem
                    entry = self._load_from_disk(key)
                    if entry and self._is_expired(entry, ttl):
                        cache_file.unlink()
                        disk_expired += 1
                except Exception as e:
                    logger.error(f"Ø®Ø·Ø£ ÙÙŠ ÙØ­Øµ Ø§Ù†ØªÙ‡Ø§Ø¡ ØµÙ„Ø§Ø­ÙŠØ© {cache_file}: {e}")
            
            logger.info(f"ØªÙ… Ø­Ø°Ù {len(expired_keys)} Ù…Ø¯Ø®Ù„ Ù…Ù† Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ùˆ {disk_expired} Ù…Ù† Ø§Ù„Ù‚Ø±Øµ")

# Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø«ÙŠÙ„ Ø¹Ø§Ù… Ù„Ù„Ù†Ø¸Ø§Ù…
cache_system = AdvancedCacheSystem()

# Ø¯ÙˆØ§Ù„ Ù…Ø³Ø§Ø¹Ø¯Ø© Ù„Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø³Ù‡Ù„
def cache_page(book_id: str, page_num: int, content: str, ttl: int = 3600):
    """ØªØ®Ø²ÙŠÙ† ØµÙØ­Ø© Ù…Ø¤Ù‚ØªØ§Ù‹"""
    identifier = f"page_{book_id}_{page_num}"
    cache_system.set(identifier, content, ttl)

def get_cached_page(book_id: str, page_num: int) -> Optional[str]:
    """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ ØµÙØ­Ø© Ù…Ø®Ø²Ù†Ø© Ù…Ø¤Ù‚ØªØ§Ù‹"""
    identifier = f"page_{book_id}_{page_num}"
    return cache_system.get(identifier)

def cache_book_metadata(book_id: str, metadata: Dict, ttl: int = 7200):
    """ØªØ®Ø²ÙŠÙ† Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ÙƒØªØ§Ø¨ Ø§Ù„ÙˆØµÙÙŠØ© Ù…Ø¤Ù‚ØªØ§Ù‹"""
    identifier = f"book_meta_{book_id}"
    cache_system.set(identifier, metadata, ttl)

def get_cached_book_metadata(book_id: str) -> Optional[Dict]:
    """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ÙƒØªØ§Ø¨ Ø§Ù„ÙˆØµÙÙŠØ© Ø§Ù„Ù…Ø®Ø²Ù†Ø© Ù…Ø¤Ù‚ØªØ§Ù‹"""
    identifier = f"book_meta_{book_id}"
    return cache_system.get(identifier)

def cache_search_results(query: str, results: List, ttl: int = 1800):
    """ØªØ®Ø²ÙŠÙ† Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ø¨Ø­Ø« Ù…Ø¤Ù‚ØªØ§Ù‹"""
    identifier = f"search_{hashlib.md5(query.encode()).hexdigest()}"
    cache_system.set(identifier, results, ttl)

def get_cached_search_results(query: str) -> Optional[List]:
    """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ù…Ø®Ø²Ù†Ø© Ù…Ø¤Ù‚ØªØ§Ù‹"""
    identifier = f"search_{hashlib.md5(query.encode()).hexdigest()}"
    return cache_system.get(identifier)

def clear_cache():
    """Ù…Ø³Ø­ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø®Ø²Ù†Ø© Ù…Ø¤Ù‚ØªØ§Ù‹"""
    cache_system.clear()

def get_cache_stats() -> Dict[str, Any]:
    """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª"""
    stats = cache_system.get_stats()
    
    # Ø¥Ø¶Ø§ÙØ© Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø¥Ø¶Ø§ÙÙŠØ©
    total_items = stats.get('memory_entries', 0)
    cache_size_mb = stats.get('memory_usage_mb', 0)
    hit_rate = stats.get('hit_rate_percent', 0)
    total_requests = stats.get('total_requests', 0)
    cache_hits = stats.get('hits', 0)
    cache_misses = stats.get('misses', 0)
    
    # Ø­Ø³Ø§Ø¨ Ø¹Ø¯Ø¯ Ø§Ù„ØµÙØ­Ø§Øª ÙˆØ§Ù„ÙƒØªØ¨ Ø§Ù„Ù…Ø®Ø²Ù†Ø© (ØªÙ‚Ø¯ÙŠØ±)
    pages_cached = sum(1 for key in cache_system.memory_cache.keys() if key.startswith('page_'))
    books_cached = sum(1 for key in cache_system.memory_cache.keys() if key.startswith('book_meta_'))
    
    return {
        'total_items': total_items,
        'cache_size_mb': cache_size_mb,
        'pages_cached': pages_cached,
        'books_cached': books_cached,
        'hit_rate': hit_rate,
        'total_requests': total_requests,
        'cache_hits': cache_hits,
        'cache_misses': cache_misses,
        **stats
    }

if __name__ == "__main__":
    # Ø§Ø®ØªØ¨Ø§Ø± Ø³Ø±ÙŠØ¹ Ù„Ù„Ù†Ø¸Ø§Ù…
    print("ğŸ§ª Ø§Ø®ØªØ¨Ø§Ø± Ù†Ø¸Ø§Ù… Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª...")
    
    # Ø§Ø®ØªØ¨Ø§Ø± ØªØ®Ø²ÙŠÙ† ÙˆØ§Ø³ØªØ±Ø¬Ø§Ø¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
    test_data = "Ù‡Ø°Ø§ Ù†Øµ ØªØ¬Ø±ÙŠØ¨ÙŠ Ù„Ù„Ø§Ø®ØªØ¨Ø§Ø±" * 100
    cache_system.set("test_key", test_data)
    
    retrieved = cache_system.get("test_key")
    print(f"âœ… Ù†Ø¬Ø­ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±: {retrieved == test_data}")
    
    # Ø¹Ø±Ø¶ Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
    stats = cache_system.get_stats()
    print(f"ğŸ“Š Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª: {stats}")
    
    print("âœ… ØªÙ… Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ù†Ø¸Ø§Ù… Ø¨Ù†Ø¬Ø§Ø­!")