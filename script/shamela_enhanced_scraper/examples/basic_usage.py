#!/usr/bin/env python3
"""
Ø£Ù…Ø«Ù„Ø© Ø£Ø³Ø§Ø³ÙŠØ© Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø³ÙƒØ±Ø¨Øª Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…ÙƒØªØ¨Ø© Ø§Ù„Ø´Ø§Ù…Ù„Ø© Ø§Ù„Ù…Ø­Ø³Ù†
Basic Usage Examples for Enhanced Shamela Scraper
"""

import asyncio
import json
from pathlib import Path
import sys

# Ø¥Ø¶Ø§ÙØ© Ù…Ø³Ø§Ø± Ø§Ù„Ø³ÙƒØ±Ø¨Øª Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ
sys.path.append(str(Path(__file__).parent.parent))

from shamela_scraper_enhanced import (
    scrape_book, 
    EnhancedShamelaExtractor,
    ShamelaScraperError
)

async def example_1_extract_full_book():
    """Ù…Ø«Ø§Ù„ 1: Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙƒØªØ§Ø¨ ÙƒØ§Ù…Ù„ Ù…Ø¹ Ø­ÙØ¸ ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª"""
    print("ğŸ”¹ Ù…Ø«Ø§Ù„ 1: Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙƒØªØ§Ø¨ ÙƒØ§Ù…Ù„")
    print("-" * 40)
    
    try:
        book = await scrape_book('1680', save_to_db=True)
        
        print(f"âœ… ØªÙ… Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙƒØªØ§Ø¨: {book.title}")
        print(f"ğŸ“„ Ø¹Ø¯Ø¯ Ø§Ù„ØµÙØ­Ø§Øª: {book.pages_count}")
        print(f"ğŸ“š Ø¹Ø¯Ø¯ Ø§Ù„Ø£Ø¬Ø²Ø§Ø¡: {book.volumes_count}")
        print(f"ğŸ“‘ Ø¹Ø¯Ø¯ Ø§Ù„ÙØµÙˆÙ„: {len(book.chapters)}")
        
    except ShamelaScraperError as e:
        print(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬: {e}")

async def example_2_extract_without_saving():
    """Ù…Ø«Ø§Ù„ 2: Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¨Ø¯ÙˆÙ† Ø­ÙØ¸ ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª"""
    print("\nğŸ”¹ Ù…Ø«Ø§Ù„ 2: Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¨Ø¯ÙˆÙ† Ø­ÙØ¸ ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª")
    print("-" * 40)
    
    try:
        book = await scrape_book('30151', save_to_db=False)
        
        print(f"âœ… ØªÙ… Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙƒØªØ§Ø¨: {book.title}")
        print(f"ğŸ‘¤ Ø§Ù„Ù…Ø¤Ù„Ù: {book.authors[0].full_name if book.authors else 'ØºÙŠØ± Ù…Ø­Ø¯Ø¯'}")
        print(f"ğŸ¢ Ø§Ù„Ù†Ø§Ø´Ø±: {book.publisher.name if book.publisher else 'ØºÙŠØ± Ù…Ø­Ø¯Ø¯'}")
        
        # Ø­ÙØ¸ ÙÙŠ Ù…Ù„Ù JSON
        output_file = Path("extracted_book.json")
        book_data = {
            'title': book.title,
            'shamela_id': book.shamela_id,
            'pages_count': book.pages_count,
            'volumes_count': book.volumes_count,
            'authors': [author.full_name for author in book.authors],
            'publisher': book.publisher.name if book.publisher else None,
            'description': book.description[:500] + '...' if book.description else None
        }
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(book_data, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ’¾ ØªÙ… Ø­ÙØ¸ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„ÙƒØªØ§Ø¨ ÙÙŠ: {output_file}")
        
    except ShamelaScraperError as e:
        print(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬: {e}")

async def example_3_extract_components():
    """Ù…Ø«Ø§Ù„ 3: Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ÙƒÙˆÙ†Ø§Øª Ù…Ø­Ø¯Ø¯Ø© Ù…Ù† Ø§Ù„ÙƒØªØ§Ø¨"""
    print("\nğŸ”¹ Ù…Ø«Ø§Ù„ 3: Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ÙƒÙˆÙ†Ø§Øª Ù…Ø­Ø¯Ø¯Ø©")
    print("-" * 40)
    
    book_id = '1680'
    
    try:
        async with EnhancedShamelaExtractor() as extractor:
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¨Ø·Ø§Ù‚Ø© Ø§Ù„ÙƒØªØ§Ø¨
            print("ğŸ“‹ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¨Ø·Ø§Ù‚Ø© Ø§Ù„ÙƒØªØ§Ø¨...")
            card = await extractor.extract_book_card(book_id)
            print(f"   Ø§Ù„Ø¹Ù†ÙˆØ§Ù†: {card.title}")
            print(f"   Ø§Ù„Ù…Ø¤Ù„Ù: {card.author}")
            print(f"   ØªØ±Ù‚ÙŠÙ… Ù…ÙˆØ§ÙÙ‚ Ù„Ù„Ù…Ø·Ø¨ÙˆØ¹: {card.has_original_pagination}")
            
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙÙ‡Ø±Ø³
            print("\nğŸ“‘ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙÙ‡Ø±Ø³...")
            chapters = await extractor.extract_book_index(book_id)
            print(f"   Ø¹Ø¯Ø¯ Ø§Ù„ÙØµÙˆÙ„: {len(chapters)}")
            for i, chapter in enumerate(chapters[:3], 1):
                print(f"   {i}. {chapter.title} (Øµ {chapter.page_start})")
            
            # Ø§ÙƒØªØ´Ø§Ù Ø§Ù„Ø£Ø¬Ø²Ø§Ø¡
            print("\nğŸ“š Ø§ÙƒØªØ´Ø§Ù Ø§Ù„Ø£Ø¬Ø²Ø§Ø¡...")
            volumes, max_page = await extractor.detect_volumes_and_pages(book_id)
            print(f"   Ø¹Ø¯Ø¯ Ø§Ù„Ø£Ø¬Ø²Ø§Ø¡: {len(volumes)}")
            print(f"   Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„ØµÙØ­Ø§Øª: {max_page}")
            
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¹ÙŠÙ†Ø© Ù…Ù† Ø§Ù„ØµÙØ­Ø§Øª
            print("\nğŸ“„ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¹ÙŠÙ†Ø© Ù…Ù† Ø§Ù„ØµÙØ­Ø§Øª...")
            pages = await extractor.extract_pages_batch(book_id, 1, 5)
            print(f"   ØªÙ… Ø§Ø³ØªØ®Ø±Ø§Ø¬ {len(pages)} ØµÙØ­Ø©")
            if pages:
                print(f"   Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ØµÙØ­Ø© Ø§Ù„Ø£ÙˆÙ„Ù‰: {pages[0].content[:100]}...")
    
    except ShamelaScraperError as e:
        print(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬: {e}")

async def example_4_batch_processing():
    """Ù…Ø«Ø§Ù„ 4: Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¯ÙØ¹ÙŠØ© Ù„Ø¹Ø¯Ø© ÙƒØªØ¨"""
    print("\nğŸ”¹ Ù…Ø«Ø§Ù„ 4: Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¯ÙØ¹ÙŠØ© Ù„Ø¹Ø¯Ø© ÙƒØªØ¨")
    print("-" * 40)
    
    book_ids = ['1680', '30151']  # Ù‚Ø§Ø¦Ù…Ø© Ù…Ø¹Ø±ÙØ§Øª Ø§Ù„ÙƒØªØ¨
    results = []
    
    for book_id in book_ids:
        try:
            print(f"\nğŸ“– Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ÙƒØªØ§Ø¨ {book_id}...")
            
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¨Ø·Ø§Ù‚Ø© Ø§Ù„ÙƒØªØ§Ø¨ ÙÙ‚Ø· Ù„Ù„Ø³Ø±Ø¹Ø©
            async with EnhancedShamelaExtractor() as extractor:
                card = await extractor.extract_book_card(book_id)
                volumes, max_page = await extractor.detect_volumes_and_pages(book_id)
            
            result = {
                'shamela_id': book_id,
                'title': card.title,
                'author': card.author,
                'pages_count': max_page,
                'volumes_count': len(volumes),
                'has_original_pagination': card.has_original_pagination,
                'status': 'success'
            }
            
            print(f"   âœ… {card.title} - {max_page} ØµÙØ­Ø©")
            
        except ShamelaScraperError as e:
            result = {
                'shamela_id': book_id,
                'status': 'failed',
                'error': str(e)
            }
            print(f"   âŒ ÙØ´Ù„: {e}")
        
        results.append(result)
    
    # Ø­ÙØ¸ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
    output_file = Path("batch_results.json")
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ’¾ ØªÙ… Ø­ÙØ¸ Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¯ÙØ¹ÙŠØ© ÙÙŠ: {output_file}")

async def example_5_error_handling():
    """Ù…Ø«Ø§Ù„ 5: Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø£Ø®Ø·Ø§Ø¡"""
    print("\nğŸ”¹ Ù…Ø«Ø§Ù„ 5: Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø£Ø®Ø·Ø§Ø¡")
    print("-" * 40)
    
    # Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙƒØªØ§Ø¨ ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯
    invalid_book_id = '999999'
    
    try:
        print(f"Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙƒØªØ§Ø¨ ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯: {invalid_book_id}")
        book = await scrape_book(invalid_book_id, save_to_db=False)
        print(f"âœ… ØªÙ… Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙƒØªØ§Ø¨: {book.title}")
        
    except ShamelaScraperError as e:
        print(f"âŒ Ø®Ø·Ø£ Ù…ØªÙˆÙ‚Ø¹ ÙÙŠ Ø§Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬: {e}")
        print("   Ù‡Ø°Ø§ Ø®Ø·Ø£ Ø·Ø¨ÙŠØ¹ÙŠ Ø¹Ù†Ø¯ Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙƒØªØ§Ø¨ ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯")
        
    except Exception as e:
        print(f"âŒ Ø®Ø·Ø£ ØºÙŠØ± Ù…ØªÙˆÙ‚Ø¹: {e}")

async def main():
    """ØªØ´ØºÙŠÙ„ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø£Ù…Ø«Ù„Ø©"""
    print("ğŸš€ Ø£Ù…Ø«Ù„Ø© Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø³ÙƒØ±Ø¨Øª Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…ÙƒØªØ¨Ø© Ø§Ù„Ø´Ø§Ù…Ù„Ø© Ø§Ù„Ù…Ø­Ø³Ù†")
    print("=" * 60)
    
    # ØªØ´ØºÙŠÙ„ Ø§Ù„Ø£Ù…Ø«Ù„Ø©
    await example_1_extract_full_book()
    await example_2_extract_without_saving()
    await example_3_extract_components()
    await example_4_batch_processing()
    await example_5_error_handling()
    
    print("\n" + "=" * 60)
    print("âœ… ØªÙ… ØªØ´ØºÙŠÙ„ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø£Ù…Ø«Ù„Ø© Ø¨Ù†Ø¬Ø§Ø­!")

if __name__ == "__main__":
    asyncio.run(main())