#!/usr/bin/env python3
"""
ÙˆØ§Ø¬Ù‡Ø© Ø³Ø·Ø± Ø§Ù„Ø£ÙˆØ§Ù…Ø± Ù„Ø³ÙƒØ±Ø¨Øª Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…ÙƒØªØ¨Ø© Ø§Ù„Ø´Ø§Ù…Ù„Ø© Ø§Ù„Ù…Ø­Ø³Ù†
Enhanced Shamela Scraper CLI Interface

Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…:
    python shamela_cli.py --book-id 30151
    python shamela_cli.py --book-id 30151 --no-save
    python shamela_cli.py --book-id 30151 --pages-only
    python shamela_cli.py --book-ids 30151,1680,5678
    python shamela_cli.py --help
"""

import argparse
import asyncio
import json
import sys
import time
from pathlib import Path
from typing import List, Optional

from shamela_scraper_enhanced import (
    scrape_book, 
    EnhancedShamelaExtractor,
    ShamelaScraperError
)

def print_banner():
    """Ø·Ø¨Ø§Ø¹Ø© Ø´Ø¹Ø§Ø± Ø§Ù„Ø³ÙƒØ±Ø¨Øª"""
    banner = """
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                    ğŸ•Œ Ø³ÙƒØ±Ø¨Øª Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…ÙƒØªØ¨Ø© Ø§Ù„Ø´Ø§Ù…Ù„Ø© Ø§Ù„Ù…Ø­Ø³Ù†                    â•‘
â•‘                        Enhanced Shamela Scraper v2.0                        â•‘
â•‘                                                                              â•‘
â•‘  âœ¨ Ù…Ù…ÙŠØ²Ø§Øª Ø§Ù„Ø³ÙƒØ±Ø¨Øª:                                                          â•‘
â•‘  â€¢ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø³Ø±ÙŠØ¹ ÙˆÙ…ØªÙˆØ§Ø²ÙŠ Ù„Ù„ØµÙØ­Ø§Øª                                            â•‘
â•‘  â€¢ Ø¯Ø¹Ù… ÙƒØ§Ù…Ù„ Ù„Ù„Ø£Ø¬Ø²Ø§Ø¡ ÙˆØ§Ù„Ù…Ø¬Ù„Ø¯Ø§Øª                                               â•‘
â•‘  â€¢ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¨Ø·Ø§Ù‚Ø© Ø§Ù„ÙƒØªØ§Ø¨ ÙˆØ§Ù„ÙÙ‡Ø±Ø³                                            â•‘
â•‘  â€¢ Ø­ÙØ¸ ØªÙ„Ù‚Ø§Ø¦ÙŠ ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª                                             â•‘
â•‘  â€¢ Ø¯Ø¹Ù… Ø§Ù„ØªØ±Ù‚ÙŠÙ… Ø§Ù„Ù…ÙˆØ§ÙÙ‚ Ù„Ù„Ù…Ø·Ø¨ÙˆØ¹                                              â•‘
â•‘  â€¢ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø°ÙƒÙŠØ© Ù„Ù„Ø£Ø®Ø·Ø§Ø¡ ÙˆØ§Ù„Ø§Ø³ØªØ«Ù†Ø§Ø¡Ø§Øª                                        â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """
    print(banner)

def format_time(seconds: float) -> str:
    """ØªÙ†Ø³ÙŠÙ‚ Ø§Ù„ÙˆÙ‚Øª Ø¨Ø´ÙƒÙ„ Ù‚Ø§Ø¨Ù„ Ù„Ù„Ù‚Ø±Ø§Ø¡Ø©"""
    if seconds < 60:
        return f"{seconds:.1f} Ø«Ø§Ù†ÙŠØ©"
    elif seconds < 3600:
        minutes = seconds / 60
        return f"{minutes:.1f} Ø¯Ù‚ÙŠÙ‚Ø©"
    else:
        hours = seconds / 3600
        return f"{hours:.1f} Ø³Ø§Ø¹Ø©"

def format_size(pages_count: int) -> str:
    """ØªÙ†Ø³ÙŠÙ‚ Ø­Ø¬Ù… Ø§Ù„ÙƒØªØ§Ø¨"""
    if pages_count < 100:
        return f"{pages_count} ØµÙØ­Ø© (ØµØºÙŠØ±)"
    elif pages_count < 500:
        return f"{pages_count} ØµÙØ­Ø© (Ù…ØªÙˆØ³Ø·)"
    elif pages_count < 1000:
        return f"{pages_count} ØµÙØ­Ø© (ÙƒØ¨ÙŠØ±)"
    else:
        return f"{pages_count} ØµÙØ­Ø© (Ø¶Ø®Ù…)"

async def scrape_single_book(
    book_id: str, 
    save_to_db: bool = True, 
    pages_only: bool = False,
    output_dir: Optional[str] = None
) -> bool:
    """Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙƒØªØ§Ø¨ ÙˆØ§Ø­Ø¯"""
    
    print(f"\nğŸš€ Ø¨Ø¯Ø¡ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙƒØªØ§Ø¨ {book_id}")
    print("=" * 60)
    
    start_time = time.time()
    
    try:
        if pages_only:
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ØµÙØ­Ø§Øª ÙÙ‚Ø·
            async with EnhancedShamelaExtractor() as extractor:
                print("ğŸ“„ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ØµÙØ­Ø§Øª ÙÙ‚Ø·...")
                volumes, max_page = await extractor.detect_volumes_and_pages(book_id)
                pages = await extractor.extract_pages_batch(book_id, 1, max_page)
                
                print(f"âœ… ØªÙ… Ø§Ø³ØªØ®Ø±Ø§Ø¬ {len(pages)} ØµÙØ­Ø©")
                
                if output_dir:
                    # Ø­ÙØ¸ Ø§Ù„ØµÙØ­Ø§Øª ÙÙŠ Ù…Ù„Ù JSON
                    output_path = Path(output_dir) / f"book_{book_id}_pages.json"
                    pages_data = [
                        {
                            'page_number': page.page_number,
                            'content': page.content,
                            'volume_number': page.volume_number
                        }
                        for page in pages
                    ]
                    
                    with open(output_path, 'w', encoding='utf-8') as f:
                        json.dump(pages_data, f, ensure_ascii=False, indent=2)
                    
                    print(f"ğŸ’¾ ØªÙ… Ø­ÙØ¸ Ø§Ù„ØµÙØ­Ø§Øª ÙÙŠ: {output_path}")
        else:
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙƒØªØ§Ø¨ ÙƒØ§Ù…Ù„Ø§Ù‹
            book = await scrape_book(book_id, save_to_db=save_to_db)
            
            elapsed_time = time.time() - start_time
            
            print("\nâœ… ØªÙ… Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙƒØªØ§Ø¨ Ø¨Ù†Ø¬Ø§Ø­!")
            print(f"ğŸ“– Ø§Ù„Ø¹Ù†ÙˆØ§Ù†: {book.title}")
            print(f"ğŸ‘¤ Ø§Ù„Ù…Ø¤Ù„Ù: {book.authors[0].full_name if book.authors else 'ØºÙŠØ± Ù…Ø­Ø¯Ø¯'}")
            print(f"ğŸ¢ Ø§Ù„Ù†Ø§Ø´Ø±: {book.publisher.name if book.publisher else 'ØºÙŠØ± Ù…Ø­Ø¯Ø¯'}")
            print(f"ğŸ“„ Ø¹Ø¯Ø¯ Ø§Ù„ØµÙØ­Ø§Øª: {format_size(book.pages_count)}")
            print(f"ğŸ“š Ø¹Ø¯Ø¯ Ø§Ù„Ø£Ø¬Ø²Ø§Ø¡: {book.volumes_count}")
            print(f"ğŸ“‘ Ø¹Ø¯Ø¯ Ø§Ù„ÙØµÙˆÙ„: {len(book.chapters)}")
            print(f"ğŸ’¾ Ø§Ù„ØµÙØ­Ø§Øª Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬Ø©: {len(book.pages)}")
            print(f"âœ¨ ØªØ±Ù‚ÙŠÙ… Ù…ÙˆØ§ÙÙ‚ Ù„Ù„Ù…Ø·Ø¨ÙˆØ¹: {'Ù†Ø¹Ù…' if book.card_info and book.card_info.has_original_pagination else 'Ù„Ø§'}")
            print(f"â±ï¸ Ø§Ù„ÙˆÙ‚Øª Ø§Ù„Ù…Ø³ØªØºØ±Ù‚: {format_time(elapsed_time)}")
            print(f"ğŸ—„ï¸ Ø­ÙÙØ¸ ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {'Ù†Ø¹Ù…' if save_to_db else 'Ù„Ø§'}")
            
            if output_dir:
                # Ø­ÙØ¸ Ù…Ù„Ø®Øµ Ø§Ù„ÙƒØªØ§Ø¨
                output_path = Path(output_dir) / f"book_{book_id}_summary.json"
                summary = {
                    'shamela_id': book.shamela_id,
                    'title': book.title,
                    'authors': [author.full_name for author in book.authors],
                    'publisher': book.publisher.name if book.publisher else None,
                    'pages_count': book.pages_count,
                    'volumes_count': book.volumes_count,
                    'chapters_count': len(book.chapters),
                    'extracted_pages': len(book.pages),
                    'has_original_pagination': book.card_info.has_original_pagination if book.card_info else False,
                    'extraction_time': elapsed_time,
                    'saved_to_db': save_to_db
                }
                
                with open(output_path, 'w', encoding='utf-8') as f:
                    json.dump(summary, f, ensure_ascii=False, indent=2)
                
                print(f"ğŸ“‹ ØªÙ… Ø­ÙØ¸ Ù…Ù„Ø®Øµ Ø§Ù„ÙƒØªØ§Ø¨ ÙÙŠ: {output_path}")
        
        return True
        
    except ShamelaScraperError as e:
        print(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙƒØªØ§Ø¨ {book_id}: {e}")
        return False
    except Exception as e:
        print(f"âŒ Ø®Ø·Ø£ ØºÙŠØ± Ù…ØªÙˆÙ‚Ø¹ ÙÙŠ Ø§Ù„ÙƒØªØ§Ø¨ {book_id}: {e}")
        return False

async def scrape_multiple_books(
    book_ids: List[str], 
    save_to_db: bool = True, 
    pages_only: bool = False,
    output_dir: Optional[str] = None
) -> None:
    """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¹Ø¯Ø© ÙƒØªØ¨"""
    
    print(f"\nğŸš€ Ø¨Ø¯Ø¡ Ø§Ø³ØªØ®Ø±Ø§Ø¬ {len(book_ids)} ÙƒØªØ§Ø¨")
    print("=" * 60)
    
    total_start_time = time.time()
    successful = 0
    failed = 0
    
    for i, book_id in enumerate(book_ids, 1):
        print(f"\nğŸ“š Ø§Ù„ÙƒØªØ§Ø¨ {i}/{len(book_ids)}: {book_id}")
        
        success = await scrape_single_book(book_id, save_to_db, pages_only, output_dir)
        
        if success:
            successful += 1
        else:
            failed += 1
        
        # ÙØªØ±Ø© Ø±Ø§Ø­Ø© Ù‚ØµÙŠØ±Ø© Ø¨ÙŠÙ† Ø§Ù„ÙƒØªØ¨
        if i < len(book_ids):
            print("â³ ÙØªØ±Ø© Ø±Ø§Ø­Ø© Ù‚ØµÙŠØ±Ø©...")
            await asyncio.sleep(2)
    
    total_elapsed = time.time() - total_start_time
    
    print("\n" + "=" * 60)
    print("ğŸ“Š Ù…Ù„Ø®Øµ Ø§Ù„Ø¹Ù…Ù„ÙŠØ©:")
    print(f"âœ… Ù†Ø¬Ø­: {successful} ÙƒØªØ§Ø¨")
    print(f"âŒ ÙØ´Ù„: {failed} ÙƒØªØ§Ø¨")
    print(f"â±ï¸ Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„ÙˆÙ‚Øª: {format_time(total_elapsed)}")
    print(f"ğŸ“ˆ Ù…ØªÙˆØ³Ø· Ø§Ù„ÙˆÙ‚Øª Ù„ÙƒÙ„ ÙƒØªØ§Ø¨: {format_time(total_elapsed / len(book_ids))}")

def main():
    """Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©"""
    parser = argparse.ArgumentParser(
        description="Ø³ÙƒØ±Ø¨Øª Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…ÙƒØªØ¨Ø© Ø§Ù„Ø´Ø§Ù…Ù„Ø© Ø§Ù„Ù…Ø­Ø³Ù†",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Ø£Ù…Ø«Ù„Ø© Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…:
  %(prog)s --book-id 30151                    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙƒØªØ§Ø¨ ÙˆØ§Ø­Ø¯
  %(prog)s --book-id 30151 --no-save          # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¨Ø¯ÙˆÙ† Ø­ÙØ¸ ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
  %(prog)s --book-id 30151 --pages-only       # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ØµÙØ­Ø§Øª ÙÙ‚Ø·
  %(prog)s --book-ids 30151,1680,5678         # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¹Ø¯Ø© ÙƒØªØ¨
  %(prog)s --book-id 30151 --output ./output  # Ø­ÙØ¸ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ ÙÙŠ Ù…Ø¬Ù„Ø¯ Ù…Ø­Ø¯Ø¯
        """
    )
    
    # Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
    group = parser.add_mutually_exclusive_group(required=True)
    group.add_argument(
        '--book-id', 
        type=str,
        help='Ù…Ø¹Ø±Ù Ø§Ù„ÙƒØªØ§Ø¨ ÙÙŠ Ø§Ù„Ù…ÙƒØªØ¨Ø© Ø§Ù„Ø´Ø§Ù…Ù„Ø©'
    )
    group.add_argument(
        '--book-ids', 
        type=str,
        help='Ù‚Ø§Ø¦Ù…Ø© Ù…Ø¹Ø±ÙØ§Øª Ø§Ù„ÙƒØªØ¨ Ù…ÙØµÙˆÙ„Ø© Ø¨ÙÙˆØ§ØµÙ„ (Ù…Ø«Ø§Ù„: 30151,1680,5678)'
    )
    
    # Ø®ÙŠØ§Ø±Ø§Øª Ø§Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬
    parser.add_argument(
        '--no-save', 
        action='store_true',
        help='Ø¹Ø¯Ù… Ø­ÙØ¸ Ø§Ù„ÙƒØªØ§Ø¨ ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª'
    )
    parser.add_argument(
        '--pages-only', 
        action='store_true',
        help='Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ØµÙØ­Ø§Øª ÙÙ‚Ø· (Ø¨Ø¯ÙˆÙ† Ø¨Ø·Ø§Ù‚Ø© Ø§Ù„ÙƒØªØ§Ø¨ ÙˆØ§Ù„ÙÙ‡Ø±Ø³)'
    )
    parser.add_argument(
        '--output', 
        type=str,
        help='Ù…Ø¬Ù„Ø¯ Ø­ÙØ¸ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ (Ø§Ø®ØªÙŠØ§Ø±ÙŠ)'
    )
    
    # Ø®ÙŠØ§Ø±Ø§Øª Ø¥Ø¶Ø§ÙÙŠØ©
    parser.add_argument(
        '--quiet', 
        action='store_true',
        help='ØªØ´ØºÙŠÙ„ ØµØ§Ù…Øª (Ø¨Ø¯ÙˆÙ† Ø·Ø¨Ø§Ø¹Ø© Ø§Ù„Ø´Ø¹Ø§Ø±)'
    )
    
    args = parser.parse_args()
    
    # Ø·Ø¨Ø§Ø¹Ø© Ø§Ù„Ø´Ø¹Ø§Ø±
    if not args.quiet:
        print_banner()
    
    # Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø¬Ù„Ø¯ Ø§Ù„Ø¥Ø®Ø±Ø§Ø¬ Ø¥Ø°Ø§ Ù„Ø²Ù… Ø§Ù„Ø£Ù…Ø±
    if args.output:
        output_path = Path(args.output)
        output_path.mkdir(parents=True, exist_ok=True)
        print(f"ğŸ“ Ù…Ø¬Ù„Ø¯ Ø§Ù„Ø¥Ø®Ø±Ø§Ø¬: {output_path.absolute()}")
    
    # ØªØ­Ø¯ÙŠØ¯ Ù…Ø§ Ø¥Ø°Ø§ ÙƒØ§Ù† Ø³ÙŠØªÙ… Ø§Ù„Ø­ÙØ¸ ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
    save_to_db = not args.no_save
    
    try:
        if args.book_id:
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙƒØªØ§Ø¨ ÙˆØ§Ø­Ø¯
            asyncio.run(scrape_single_book(
                args.book_id, 
                save_to_db, 
                args.pages_only,
                args.output
            ))
        elif args.book_ids:
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¹Ø¯Ø© ÙƒØªØ¨
            book_ids = [bid.strip() for bid in args.book_ids.split(',') if bid.strip()]
            if not book_ids:
                print("âŒ Ø®Ø·Ø£: Ù‚Ø§Ø¦Ù…Ø© Ù…Ø¹Ø±ÙØ§Øª Ø§Ù„ÙƒØªØ¨ ÙØ§Ø±ØºØ©")
                sys.exit(1)
            
            asyncio.run(scrape_multiple_books(
                book_ids, 
                save_to_db, 
                args.pages_only,
                args.output
            ))
    
    except KeyboardInterrupt:
        print("\nâš ï¸ ØªÙ… Ø¥ÙŠÙ‚Ø§Ù Ø§Ù„Ø¹Ù…Ù„ÙŠØ© Ø¨ÙˆØ§Ø³Ø·Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…")
        sys.exit(1)
    except Exception as e:
        print(f"\nâŒ Ø®Ø·Ø£ ØºÙŠØ± Ù…ØªÙˆÙ‚Ø¹: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()