# خطة سحب البيانات من المكتبة المتكاملة

## نظرة عامة

هذه الوثيقة تحتوي على خطة تفصيلية لسحب جميع المؤلفين والأقسام والكتب من موقع المكتبة المتكاملة (https://web.mutakamela.org/) وإدراجها في قاعدة البيانات المحلية.

## تحليل الموقع المصدر

### معلومات عامة عن المكتبة المتكاملة
- **الموقع**: https://web.mutakamela.org/
- **النوع**: مكتبة إسلامية رقمية
- **المحتوى**: مئات من المراجع والمصادر الإسلامية
- **التنسيق**: كتب إلكترونية بصيغ مختلفة
- **اللغة**: العربية بشكل أساسي

### التحديات المتوقعة
1. **الحماية من البوتات**: قد يحتوي الموقع على آليات حماية ضد الـ web scraping
2. **التحميل الديناميكي**: قد يستخدم JavaScript لتحميل المحتوى
3. **معدل الطلبات**: قد يكون هناك حدود على عدد الطلبات في الثانية
4. **هيكل البيانات**: قد يكون معقداً ويتطلب تحليل دقيق

## تحليل قاعدة البيانات المحلية

### النماذج الموجودة

#### 1. نموذج المؤلف (Author)
```php
protected $fillable = [
    'full_name',
    'biography',
    'image',
    'madhhab',
    'is_living',
    'birth_year_type',
    'birth_year',
    'death_year_type',
    'death_year',
    'birth_date',
    'death_date',
];
```

#### 2. نموذج الكتاب (Book)
```php
protected $fillable = [
    'title',
    'description',
    'slug',
    'cover_image',
    'published_year',
    'publisher',
    'publisher_id',
    'pages_count',
    'volumes_count',
    'status',
    'visibility',
    'cover_image_url',
    'source_url',
    'book_section_id',
];
```

#### 3. نموذج القسم (BookSection)
```php
protected $fillable = [
    'name',
    'description',
    'parent_id',
    'sort_order',
    'is_active',
    'slug',
    'logo_path',
];
```

### العلاقات
- **المؤلف والكتاب**: علاقة many-to-many عبر جدول `author_book`
- **الكتاب والقسم**: علاقة one-to-many
- **الأقسام**: علاقة هرمية (parent-child)

## الخطة التقنية

### المرحلة الأولى: التحليل والاستطلاع

#### 1. فحص هيكل الموقع
- تحليل صفحات الموقع الرئيسية
- فهم نظام التصنيف والتنظيم
- تحديد نقاط النهاية (endpoints) المتاحة
- فحص إمكانية وجود API

#### 2. تحديد استراتيجية السحب
- **الأدوات المقترحة**:
  - Python مع مكتبات BeautifulSoup و Requests
  - Selenium للمواقع التي تستخدم JavaScript
  - Scrapy للمشاريع الكبيرة

### المرحلة الثانية: تطوير أدوات السحب

#### 1. إعداد البيئة
```python
# المكتبات المطلوبة
import requests
from bs4 import BeautifulSoup
import time
import json
import mysql.connector
from selenium import webdriver
import logging
```

#### 2. بناء الـ Scrapers

##### أ. سحب الأقسام
```python
class CategoryScraper:
    def __init__(self):
        self.base_url = "https://web.mutakamela.org"
        self.session = requests.Session()
    
    def get_categories(self):
        # منطق سحب الأقسام
        pass
    
    def parse_category(self, category_data):
        # تحليل بيانات القسم
        pass
```

##### ب. سحب المؤلفين
```python
class AuthorScraper:
    def get_authors(self):
        # منطق سحب المؤلفين
        pass
    
    def parse_author(self, author_data):
        # تحليل بيانات المؤلف
        pass
```

##### ج. سحب الكتب
```python
class BookScraper:
    def get_books(self):
        # منطق سحب الكتب
        pass
    
    def parse_book(self, book_data):
        # تحليل بيانات الكتاب
        pass
```

### المرحلة الثالثة: معالجة البيانات

#### 1. تنظيف البيانات
- إزالة HTML tags
- توحيد تنسيق التواريخ
- معالجة الأسماء المكررة
- تصحيح الترميز (encoding)

#### 2. التحقق من صحة البيانات
- التأكد من وجود الحقول المطلوبة
- التحقق من صحة التواريخ
- فحص الروابط

### المرحلة الرابعة: الإدراج في قاعدة البيانات

#### 1. إعداد الاتصال
```python
class DatabaseManager:
    def __init__(self):
        self.connection = mysql.connector.connect(
            host='localhost',
            database='your_database',
            user='your_username',
            password='your_password'
        )
    
    def insert_author(self, author_data):
        # إدراج المؤلف
        pass
    
    def insert_book(self, book_data):
        # إدراج الكتاب
        pass
    
    def insert_category(self, category_data):
        # إدراج القسم
        pass
```

#### 2. معالجة التكرار
- فحص وجود السجلات قبل الإدراج
- استخدام unique constraints
- تحديث البيانات الموجودة عند الحاجة

## التحديات والحلول

### 1. الحماية من البوتات
**الحلول**:
- استخدام User-Agent headers واقعية
- إضافة تأخير بين الطلبات
- استخدام proxy servers
- محاكاة سلوك المستخدم الطبيعي

### 2. معدل الطلبات
**الحلول**:
- تنفيذ rate limiting
- استخدام threading بحذر
- مراقبة استجابة الخادم

### 3. حجم البيانات
**الحلول**:
- معالجة البيانات على دفعات
- استخدام pagination
- تنفيذ نظام استئناف العملية

## تقدير الوقت والموارد

### الوقت المتوقع
- **التحليل والتخطيط**: 2-3 أيام
- **تطوير الأدوات**: 5-7 أيام
- **الاختبار والتصحيح**: 3-5 أيام
- **السحب الفعلي**: 1-3 أيام (حسب حجم البيانات)
- **المعالجة والإدراج**: 2-4 أيام

### الموارد المطلوبة
- خادم مع اتصال إنترنت مستقر
- مساحة تخزين كافية
- قاعدة بيانات محسنة للأداء

## المخاطر والتخفيف

### المخاطر المحتملة
1. **حظر IP**: قد يتم حظر عنوان IP
2. **تغيير هيكل الموقع**: قد يتغير تصميم الموقع
3. **مشاكل قانونية**: قد تكون هناك قيود على السحب
4. **فقدان البيانات**: مخاطر فقدان البيانات أثناء النقل

### استراتيجيات التخفيف
1. **استخدام VPN وتدوير IP**
2. **مراقبة مستمرة للموقع**
3. **مراجعة الشروط والأحكام**
4. **نسخ احتياطية منتظمة**

## التوصيات

### قبل البدء
1. **مراجعة قانونية**: التأكد من عدم انتهاك حقوق الطبع والنشر
2. **اتصال بالموقع**: محاولة الحصول على إذن رسمي
3. **اختبار محدود**: البدء بعينة صغيرة

### أثناء التنفيذ
1. **مراقبة مستمرة**: متابعة العملية باستمرار
2. **تسجيل مفصل**: الاحتفاظ بسجلات تفصيلية
3. **نسخ احتياطية**: إنشاء نسخ احتياطية دورية

### بعد الانتهاء
1. **فحص جودة البيانات**: التأكد من صحة البيانات المستوردة
2. **تحسين الأداء**: تحسين فهارس قاعدة البيانات
3. **توثيق العملية**: توثيق الخطوات والنتائج

## الخلاصة

سحب البيانات من المكتبة المتكاملة مشروع معقد يتطلب تخطيطاً دقيقاً وتنفيذاً حذراً. النجاح يعتمد على فهم عميق لهيكل الموقع المصدر وقاعدة البيانات المستهدفة، بالإضافة إلى تطبيق أفضل الممارسات في web scraping ومعالجة البيانات.

من المهم البدء بنهج تدريجي واختبار العملية على نطاق صغير قبل التوسع، مع الحرص على احترام قوانين حقوق الطبع والنشر وشروط استخدام الموقع.